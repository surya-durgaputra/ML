{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.psychologyinaction.org/psychology-in-action-1/2016/08/13/what-is-a-sampling-distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non sample method is that of Population census. Here every single unit is measured in the target population. This is easier for smaller target populations but incredibly expensive, difficult or sometimes impossible for larger populations.\n",
    "\n",
    "Hence we depend on population sampling. \n",
    "\n",
    "Sampling Distribution definition: If I choose every possible sample of size \"n\" from population, then I get a sampling distribution. Note: What we mean is that theoretically, we take so many samples from the population that we end up covering every single unit in population. This is the theoretical meaning of Sampling distribution. Obviously, it is impossible in reality to do this for many practical populations like petal lenghts of all virginica flowers in California. Another important thing is that Sampling distribution is assiciated with some popular statistic like mean or variance or standsrd deviation. Like, if we calculate mean of every sample, we are making a Sampling Distribution of sample means.  \n",
    "\n",
    "__Two types of Sampling__\n",
    "### Non - Probability Sampling:\n",
    "- Generally does __not__ involve random selection\n",
    "- Probabilities of selection <u>can't be determined</u> for population units since we have no control over who gets sampled. It is often opportunistic in some way like in examples below and hence can be heavily biased.\n",
    "- Examples:\n",
    "    + __opt in web surveys__ (take whoever is interested in taking that survey. We are not selecting people at random from some well defined list or sampling frame. Its whoever wants to volunteer in that survey and as a result we can't determine the probabilities of selection)  \n",
    "    + __quota sampling__ you try to recruit as many people as you can who fit certain subgroup definition (like African american males or older african american males) until you hit some targets, some number of individuals that you wish to measure. In many cases, researchers try to sample as many inidividuals as they can, not according to any probability scheme, but just on basis of whoever is available and just try to hit their quotas. This is also non-probability sampling as we can't write down the probabilities of a target being selected. We are just trying to meet targets.\n",
    "    + __snowball sampling__ This is a network sampling where a sampled individual refers a friend, who then refers a friend and so on. In this case again, friends are recruiting friends. We don't really have any control over who they recruit or the probabilities with which they are going to recruit these individuals. So snowball sampling is a convenient tool for sampling but as researchers, we have no control over probabilities of selection of those samples.\n",
    "    + __convenience sampling__ Walk down the street and talk to people who are available. Talk to friends or co-workers. Again, no probabilities of selection involved. You are just trying to collect data from individuals who are convenient and in close proximity to you. Again we can't apply probabilities of selection for these individuals and that prevents us from making representative statements about the larger population. It can heavily biased. \n",
    "\n",
    "### Probability Sampling:\n",
    "- Construct list of all units in population = __Sampling Frame__\n",
    "- Determine __probability of selection__ for every unit on the list (known and non-zero)\n",
    "- __Select units from list at random__, with the sampling rates for different subgroups determined by probabilities of selection\n",
    "- Attempt to __measure__ randomly selected units\n",
    "\n",
    "__Why prefer Probability Sampling:__ The known probabilities of selection for all units allow us to make unbiased statements about both population features (the stuff that we are trying to estimate when we analyze the data) and the uncertainity in survey estimates. So in addition to saying that what is the average income of the people, we would also like to say how uncertain we are about those estimates because we are not measuring everyone in the population, we are measuring a sample of individuals. And we want to make an estimate of how uncertain we are with the estimate we are computing.\n",
    "\n",
    "Random selection of population units __protects us against bias from the sample selection mechanism__. \n",
    "\n",
    "__Probability sampling__ allows us to make population __inferences__ about larger populations, based on __sampling distributions__. \n",
    "\n",
    "If we draw a sample from a population, and calculate our estimes from that sample, and we do this process repeatedly, over time, a distribution of estimates emerges. This is called sampling distribution. In reality, we often make only one sample and if the sample drawing was carefully and scientifically designed, we can do representative estimates about larger population from this sample. The big idea is that with careful design, probability samples yield __representative, realistic, random__ samples from larger populations; such samples have statistical properties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Sampling types:\n",
    "#### Simple Random Sampling (SRS):\n",
    "- Start with know list of N population units, and randomly select n units from the list. So n is the sample size.\n",
    "- Every unit has __equal probability of selection__ = n/N\n",
    "- All possible samples of size n are equally likely.\n",
    "- Estimates of means, proportions and totals based on SRS are __unbiased__ (equal to the population values on average!). Note that for each sample of size n drawn, there might be variability in the estimates, but on average and over a large number of samples, these estimates will be the same as true population estimates.\n",
    "- SRS can be __with replacement__ or __without replacement__\n",
    "    + for both: probability of selection for each unit is still n/N\n",
    "- SRS, though simple, is rarely used in practise. Collecting data from n randomly sampled units in a very large population can be prohibitively expensive. __Exception is when there is relatively cheap data collection based on well-defined population lists or we have a collection of file records where we can literally pull records out of file cabinets.__\n",
    "- SRS is generally done when the populations are smaller and its easier and simpler to collect the data.\n",
    "- SRS connection to __i.i.d. Data__:\n",
    "    + recall that i.i.d. observations are __independent__ and __identically distributed__\n",
    "    + SRS will generate i.i.d. data for a given variable, in theory\n",
    "        - All randomly sampled units will yield observations that are independent( not correlated to eachother) and identically distributed (representative of some larger population of value, again, in theory)\n",
    "\n",
    "Example of SRS:\n",
    "We have a list of 2500 emails received. We need to find the average response time for each email. Here, we cannot just take all 2500 emails because due to infrastructure limitations, to find the response times, we will need to go through each of the email manually. So we do SRS and our sample size is 100.\n",
    "So our sampling design for SRS is: We number emails 1 to 2500 and randomly select 100 using a random number generator.\n",
    "- __Every email has known probability of selection__ = 100/2500\n",
    "- Produces __random, representative sample__ of 100 emails (in theory)\n",
    "- __Estimated mean response time__ will be an __unbiased__ estimate of population mean\n",
    "    + this is a feature of probability sampling - __On average if we were to draw many, many samples of size 100 and compute the mean of each of those, the average of those means will be equal to the true population value.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex Probability Sampling\n",
    "- With larger populations, __complex samples__ often selected, where each sampled unit has known probability of selection. __Complex = anything more complicated than SRS!__\n",
    "- With complex probability sampling, we use very specific features of sampling design that enable us to save on cost and make our samples more efficient.\n",
    "- __Complex samples have certain key features__:\n",
    "    + Population divided into different __strata__, and part of sample is allocated to each __stratum__; this ensures sample representation from each stratum, and reduces variance of survey estimates (__stratification__)\n",
    "    + __Clusters__ of population units (e.g. counties) are randomly sampled first (with known probability) within strata, to save costs of data collection (collect data from cases close to each other geographically). We often use group, strata and cluster interchangeably. In this case, cluster within a stratum is just a multi-level cluster.\n",
    "    + __Units randomly sampled within clusters__, according to some probability of selection and measured\n",
    "- **__So a unit's probability of selection is determined by__**:\n",
    "    + Number of clusters sampled from each stratum\n",
    "    + Total number of clusters in population in each stratum\n",
    "    + Number of units ultimately sampled from each cluster\n",
    "    + Total number of units in population in each cluster\n",
    "\n",
    "Example of finding a unit's probability of selection:\n",
    "- Select **__a__** out of __A__ clusters at random in a given stratum\n",
    "- then select **__b__** out of __B__ units at random from within a selected cluster\n",
    "\n",
    "Probability of selection = $(\\frac{a}{A})(\\frac{b}{B})$\n",
    "The stratum here can be Midwest US. Clusters can be all the counties in the midwest (each county being a cluster). So people designing constraints will decide what a and b will be in accordance to the cost constraints.\n",
    "\n",
    "Example: NHANES\n",
    "- Divide U.S. into different regions based on geography and population density. We refer to these divisions as __strata__ And again, by allocating samples to each of these stratum, we ensure some representation, thus minimizing the risk of a bad simple random sample where when selecting samples totally at random, they just happened to all come from one region. This this approach ensures __increased representation__. \n",
    "- Allocate some number of counties / groups of counties to be sampled from each stratum (These are called __clusters__. We randomly sample clusters again to save costs. This way we can randomly sample households that are within a specific geographic area rather than pure SRS where the households can be from anywhere in US)\n",
    "- Sample certain socio-demographic subgroups of individuals at higher rates within counties. This is called __oversampling__. So maybe, a given project has a certain target sample size for particular subgroups of individuals. We might sample these separate subgroups at higher rates within those counties (or cluster) when we are randomly selecting households. What this leads to is different probabilities of selection for different types of individuals depending on the goals of a project. That oversampling means that different people will have different probabilities of being selected, different rates of selection at that second stage, and that's okay. We still have a probability design and we can make use of those probabilities to ultimately make representative statements about the larger population.\n",
    "\n",
    "__Stage 1__: We have entire US. We can subdivide the US into regions and then sample counties.\n",
    "\n",
    "__Stage 2__: For a county from stage 1, we then randomly choose some areas within a county. This again is a cost saving measure. This is already a multi-stage cluster (stage 1 then stage 2)\n",
    "\n",
    "__Stage 3__: For a particular area within a county from stage 2, we get a list of all the households and do a simple random selection of households.\n",
    "\n",
    "__Stage 4__: At his stage, a field member can actually visit the households from stage 3 and randomly choose a subject within that particular household (like if there are 3 people in a household, randomly choose one). We then take all the measurements from this individual as seen in NHANES dataset.\n",
    "\n",
    "At all four of these stages, we know what the probabilities of selection are. And we maintain those probabilities of selection throughout the entire design. Ultimately we can can compute the probability of being include for every single individual that we might randomly sample.\n",
    "\n",
    "In this type of design, the inverse of a person's probability of selection is then their __sampling weight__. So if my probability is 1/100, my sampling weight is 100. In other words, I represent myself and __99 others__ in the population!\n",
    "\n",
    "This sampling weight, which is a function of probability of selection, is used in the actual data analysis to compute representative population estimates. So that probability of selection plays a direct role in the computation of estimates based on complex samples.\n",
    "\n",
    "So, summarizing, \n",
    "- those weights get used to compute __unbiased estimates__ of population quantities (e.g. mean BMI), accounting for different probabilities of selection.  \n",
    "- Probabilities of selection play a __direct and essential role__ in computation of unbiased population estimates!\n",
    "\n",
    "So __WHY PROBABILITY SAMPLING__:\n",
    "- Having __known, non-zero probability of selection__ for each unit in a population and __subsequent random sampling__ ensures all units will have a chance of being sampled\n",
    "- __Probability sampling__ allows us to compute __unbiased estimates__ (using those sample wieghts), and also estimate features of __sampling distribution__ of estimates that we would see if many of the same types of probability samples were selected. We can actually simulate what that sampling distribution would look like based on selecting only one sample. This is the beauty of probability sampling. We dont have to draw samples over and over again, to get a sense of what that distribution of estimates might look like.\n",
    "\n",
    "Thus, Probability sampling provides a __statistical basis for making inferences__ about certain quantities in larger populations. We can make inferences about the entire population on the basis of just one sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Probability Samples\n",
    "\n",
    "Features of __non-probability__ samples:\n",
    "- Probabilities of selection __can't be determined__ for sampled units.\n",
    "- __No random selection__ of individual units. So we dont control the random selection mechanism that ultimately yields the sample in a Non-probability sample.\n",
    "- Sample can be divided into groups(strata) or clusters, but __clusters not randomly sampled__ in earlier stages.\n",
    "- Data collection often very cheap relative to probability sampling. This is the big advantage of non-probability samples.\n",
    "\n",
    "Example of Non-probability Sampling:\n",
    "- Study of volunteers (e.g. clinical trials):\n",
    "    + You would often see a flyer or posting online like \"Do you suffer from a disease XXX. If yes, call XXX-XXXX (phone number) and become part of the study\". And then someone calls that number to become part of this study. \n",
    "    + So here, there are no sampling frames or lists. Researchers are just looking for volunteers to join their study. They have no control over who volunteers to join the study.\n",
    "- __Opt-in__/ Intercept web surveys:\n",
    "    + So when you are on a website and you see an invitation to come and complete a survey or you see an opinion survey on a particular website and you decide to join this particular survey.\n",
    "    + Again, there is no probability of selection. There is no random selection. The people trying to collect the data are just looking for volunteers to ultimately join the survey.\n",
    "- __Snowball__ samples:\n",
    "    + This is where the sample grows by people referring others for data collection. But there are no probabilities of selection that govern who participates in the survey.\n",
    "- __Convenience__ samples:\n",
    "    + Students or faculty conduct a study and select samples from their class.\n",
    "- __Quota__ samples:\n",
    "    + You have certain targets that are needed to be met. Like recruuit 1000 males and 1000 females in any way.\n",
    "    \n",
    " \n",
    "### Common feature of Non Probability samples \n",
    " Probabilities of selection <u>cannot be determined</u> a priori (or before you begin the study)! . This is the crucial difference between probability sampling and non probability sampling.\n",
    " \n",
    "Hence in a non probability sample:\n",
    "- there is __no statistical basis for making inference__ about larger population from which sample was selected.\n",
    "- There is a strong risk of __sampling bias__ because samples units are not selected at random.\n",
    "- Sampled units __not generally representative__ of a larger target population of interest.\n",
    "\n",
    "All these make it difficult to make an unbiased estimate of the overall population.\n",
    "\n",
    "But all is not lost. It is still possible tosay something about larger populations using datasets made of non-probability samples. There are two possible approaches:\n",
    "- __Pseudo-Randomization__\n",
    "    + With some transformation of data, we can treat Non-probability sample like a probability sample.\n",
    "- __Calibration__\n",
    "    + You weight your non probability sample to look more like the population that you're interested in. So this involves weighting and other model based adjustments.\n",
    "\n",
    "These approaches can transform a non-prabability sampled data into a dataset that looks as if it came from a probability sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-Randomization Approach\n",
    "- We __combine non-probability sample with a probability sample__ (i.e. \"stack\" data sets together). We find groups of samples in the probability sample that are similar to those in the non-probability samples and literally stack them together.\n",
    "- __Estimate probability of being included in non-probability non-probability sample__ as a function of auxiliary information available in both samples\n",
    "- __Treat estimated probabilities of selection as \"known\"__ for non-probability sample, use probability sampling methods for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Approach\n",
    "- __Compute weights for responding units__ in non-probability sample that allow weighted samples to mirrow a known population.\n",
    "\n",
    "For example:\n",
    "__Non probability sample__:70% female, 50% male\n",
    "__Population__: 50% female, 50% male\n",
    "\n",
    "So, down-weight females and Up-weight males\n",
    "\n",
    "__Limitation of Calibration Approach__: If weighting factor not related to variable(s) of interest, it will not reduce possible sampling bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Example: Non-probability sample\n",
    "\n",
    "API to extract info from several hundred thousand tweets and indicator of support for President Trump computed\n",
    "- __Probability__ of a tweet being selected __cannot be determined__\n",
    "    + We just grabbed all these tweets where there was a mention of President Trump. In some sense this is a convenience sample of tweets and we can't determine the probability of one tweet being selected using this mechanism. We don't have a frame of all possible tweets. Tweets don't have probabilities of selection. We don't randomly select them. We just take whatever is available that indicated some mention of President Trump.\n",
    "    + __Twitter users are not a random sample__ of a larger population. So, people who joined twitter are interested in expressing their ideas or opinions about particular topics and people can choose to join twitter.\n",
    "    + So, __Lots of data__, but:\n",
    "        - high potential for sampling bias\n",
    "            + we have lots of data (hundreds of thousands of tweets with President Trump in them) but due to lack of probabilities of selection, no random sampling of tweets, there is a very high chance of sampling bias.\n",
    "        - lack of representation: may only capture people with strong opinions! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we will focus on __Sampling distributions and sampling variance__. We will see how to estimate features of these distributions <u>based on only one probability sample</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Sampling Distribution\n",
    "\n",
    "Say we sample all the people in a country of their weight (a census survey). We then plot all the values of weight we got on a chart (distplot in seaborn with weights on x axis and freq on y axis). This is called a distribution and it shows the distribution of weights of citizens of that country.\n",
    "These kinds of variables (like weights of citizens of a country) often follow a distribution called a Normal Distribution.\n",
    "\n",
    "So, a distribution is a __set of values on a variable of interest__.\n",
    "\n",
    "In a __sampling distribution__, we take a sample from a larger population. We __assume__ that values on variable of interest would follow certain distribution <u>if we could measure entire population</u>. \n",
    "\n",
    "Hence, when we select probability samples to make inferential statements about larger populations, we refer to __sampling distribution__.\n",
    "\n",
    "#### <font color=red>It is very important to differentiate a sampling distribution from a distribution of values on a variable of interest.\n",
    "\n",
    "__Sampling Distribution__= distribution of __survey estimates__ we would see __if__ we selected many random samples using __same sampling design__, and __computed an estimate from each__.\n",
    "\n",
    "A sampling distribution is __NOT__ a distribution of values on a variable of interest. It is a distribution of survey estimates. Specifically, ampling distribution is a distribution of __survey estimates__ we would see __if__ we selected many random samples using __same sampling design__, and __computed an estimate from each__.\n",
    "\n",
    "So when we talk about Sampling distribution, we talk about distribution of values of estimates. If we had selected thousands and thousands of hypothetical and repeated random probability samples, all using the exact same probability sampling design, we will get this sampling distribution. If we had the luxury of doing that and we estimated our estimate of interest, maybe its a mean or a proportion from each of those probability samples, a distribution of estimates would arise. Just like we talk about the distribution of values on a single variable of interest, if we were to draw thousands and thousands of probability samples, we would see a distribution of estimates, where any one sample that we draw, we would compute an estimate and then the distribution would be what all of those estimates would look like if we repeated that probability sampling process over and over again.\n",
    "\n",
    "- __Key properties of Sampling Distribution__:\n",
    "    + __Hypothetical__: This distribution is hypothetical. Remember, if we had the luxury of drawing thousands of random probability samples, each one using the same design, and measuring the units in each of those probability samples and then computing an estimate based on all those measures that we collect. We do that over and over and over again and then we plot the distribution of all those estimates that would arise. This is a hypothetical thought exercise. We are not actually going to do that in practise. But the sampling distribution in theory describes what that distribution of estimates would look like if we had the luxury of doing that. But remember that it is a hypothetical exercise and we are trying to estimate the features of that sampling distribution based on one sample only.\n",
    "    + Sampling distribution is also generally very __different in appearance from distribution of values on a single variable of interest__. So we dont want to equate those two ideas.\n",
    "    + With __large enough probability sample size__, sampling distribution of estimates will look like a __normal distribution__, regardless of what estimates are being computed! __Central Limit Theorem:CLT__. This is a very attractive property of probability sampling. The larger the size of our sample and we keep drawing estimates in theory over and over again, the more that sampling distribution is going to look like a normal distribution. That makes it easy to work with these kinds of distributions. This happens because of something called __Central Limit Theorem__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Sampling Variance\n",
    "\n",
    "__Sampling Variance__ = variability in estimates described by the sampling distribution.\n",
    "\n",
    "So, hypothetically, if we were to draw thousands and thousands of those random probability samples and plotted the estimates computed form each of these probability samples, we could describe the variability of that sampling distribution and that's what we mean by __sampling variance__. How variable are the estimates that we are trying to compute across those hypothetical repeated samples.\n",
    "\n",
    "Because we select a __sample__ (and we are not including everyone in the population in that sample), a survey estimate based on a __single sample will not be exactly equal to population quantity of interest__. Remember, with probability sampling, we are selecting cases at random. So we're selecting a sub-sample of all the individuals in the larger population and in any one sample from any of these hypothetical repeated samples, that estimate that we are going to compute is not going to be exactly equal to the population quantity that we're interested in because we're taking a sub-sample of the larger overall population. And that subsample, although we want it to be representative in our probability sampling design, the estimate that we compute will not be exactly equal to the overall population value. This is what is called __Sampling Error__.\n",
    "\n",
    "The fact that we dont measure everybody in a given population and we only measure a sample of individuals in that population, the estimate based on that sample will not be exactly equal to the population value. We just want our estimates across these hypothetical repeated samples to be equal to that population quantity on average. So, if we were to average all of the estimates that we computed from these hypothetical repeated samples, any one of them is not going to be exactly equal to the population quantity that we are interested in. But the average of all those estimates (the __mean__) will be equal to that population quantity. __That's the key property of our sample design that we want and that means we have unbiased estimates__. But the idea that any one sample estimate will not be equal to the true population quantities, say the mean or the proportion we are interested in, that idea is known as __Sampling Error__.  \n",
    "\n",
    "Across hypothetical repeated samples, __these sampling errors will randomly vary__ (some positive, some negative...). In one sub-sample, the mean that we're interested in, that mean might be a little bit lower than the population quantity. Another hypothetical sample, that mean might be a little bit higher. Another hypothetical sample, that mean might be very close to the population quantity. Those estimataes are going to bounce around across these hypothetical repeated samples. Some will be negative, some positive and some will be right on the value that we are trying to estimate. But there's going to be variability in those sampling errors.\n",
    "\n",
    "The variability of those sampling errors describes the variance of the sampling distribution. If we look at that entire distribution of sample estimates, how much they bounce around the true population value that we're trying to estimate, that is the variance of that sampling distribution. And this is what we refer to as sampling variance.\n",
    "\n",
    "So, __if every sample estimate was equal to population quantity of interest__ (for eample, if our probability sample design involved taking a census, where we tried to measure everybody in the population, and we did that over and over and over again, hypothetically of course, then every single estimate will be exactly equal to the true population quantity), there would be __no__ sampling error and __no__ sampling variance.\n",
    "\n",
    "So having zero sampling error and zero sampling variance is a hypothetical ideal and that's what we try to strive for in the case of taking a population census. But as we talked abount earlier, very difficult and very expensive to do that in practise. So instead we take samples.\n",
    "\n",
    "So, with a __larger probability sample size__, sampling more from a given population, in theory there will be less sampling error and sampling errors will be less variable. Now, why is this? Think back to the idea of the census. The closer and closer we get to taking a census, the less sampling error we are going to have. So, the larger a sample size becomes, the closer we get to the population size, the less sampling error we are going to have. Though those error will still bounce around a little bit, but they will be closer to the true population value with a larger sample size. And then, the sampling errors, as a result, are going to be less variable. There's going to be less sampling variance. \n",
    "\n",
    "So this is another critical feature of trying to make inferences about populations. The bigger that your sample size becomes, the less sampling error we are going to have and the less variable our estimates are going to be. __So that Sampling distribution is going to shrink__. Its not going to be as wide as if we had a smaller sample size. \n",
    "\n",
    "__Larger__ Samples = __Less__ sampling variance. __More precise__ estimates, __more confidence__ in inferential statements (__but__ more costly). \n",
    "\n",
    "So, in sampling there is a tradeoff between cost and sampling variance.\n",
    "\n",
    "__Spread__ of sampling distribution becomes __smaller__ as sample size becomes __larger__. This just means that our estimates are less variable.\n",
    "\n",
    "Also, when we use cluster sampling and probability samples, those sampling distributions tend to spread out a little more and become more variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is sampling variance so important for making population inferences based on probability samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have introduced this idea of cost vs variance tradeoff. The larger sample costs more money and shrinks our variance. So we have to deal with these tradeoffs, data collection costing more and getting more precise estimates based on the sampling distribution.\n",
    "\n",
    "In practise, we only have the resources to select __one sample__. In theoretical world, we had this idea of repeated samples. But in the real world, we only have the resources to select one sample and then measure that sample and compute estimates.\n",
    "\n",
    "__Sampling theory__ allows us to __estimate features of sampling distribution__ (including their variance) based on __one sample__. This is one of the wonderful features of probability sampling as it gives us the statistical mechanism to actually estimate the variability of those sampling distributions when we only select one sample. So, we don't have to go out and select thousands and thousands of samples and compute estimates based on each of those samples to determine what those sampling distributions look like. The sampling theory allows us to estimate the features of those sampling distributions by only drawing one sample. This is why probability sampling is so very important for making statements about larger populations. \n",
    "\n",
    "So this is the __Magic__ of probability sampling: We select one probability sample and features of that probability sample design (the probabilities of selection for different individuals, the stratification (if we divided the population into different strata), the cluster sampling (if we selected counties and then neighborhoods and then households).. like that of NHANES), and this gives us all we need to know something about the expected sampling distribution (mainly what the variance of that sampling distribution looks like, and what the mean of that sampling distribution looks like)  .\n",
    "\n",
    "Because we can estimate variance of sampling distribution based on only one sample, we can make inferential statememts about where most estimates based on a particular sample design will fall. And this tells us something about the population. Using a particular probability sampling design, if we can make statements about the variability of our estimates and calculated the overall mean of our estimate, we can make estimates of where all of those estimates based on that probability sample design will tend to fall. And that tells us something about our population.\n",
    "\n",
    "So, __we can make statements about likely values of population parameters__ that account for variability in sampling errors that arises from random probability sampling. So we can make a statement about the overall mean that we're trying to estimate or an overall proportion that we're trying to estimate. Something that describes a feature of the target population. But in addition to computing those overall estimates based on our probability sample, we can also talk about how variable those estimates can be. How much uncertainty was there in the estimate that we're trying to compute. And being able to estimate the variance of the sampling distribution allows us to make a statement about the variability of our estimates that we're computing. \n",
    "\n",
    "Link: https://markkurzejaumich.shinyapps.io/multiple_population_bias/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Interesting result\n",
    "When you're given a large enough sample from some population, the sampling distributions of most statistics of interest will tend to normality (normal distribution), and this is regardless of how the input variables are actually distributed themselves.\n",
    "\n",
    "The Central Limit Theorem says that given a large enough sample, the distribution of estimates will tend to look like a bell shaped curve, or a normal distribution(only true for sample mean.. sampling distribution of sample variance will be a Chi-Square distribution if the population was normal, if the population was not normal, it is very difficult to define mathematically the exact shape of sampling distribution of sample varience). This result, which again is based on the Central Limit Theorem, is what drives design-based statistical inference, or what's sometimes referred to as Frequentist Statistical Inference, the idea that we can look at this sampling distribution, assuming that it's normal, and make inference about the value of a given statistic in a larger population based on that theoretical normal distribution.\n",
    "\n",
    "Some of the __common population estimates__ (also called __popular statistics__), besides mean, varaiance and standard deviation, are pearson correlation coeffecient between two variables in a sample unit, regression coefficient (the slope parameter... we plot its distribution for all samples). Again, due to CLT, if we take a large enough sample size, we see that all of the common population estimates follow normal distribution. And these distributions are centered around the true value (like if the estimate was regression slope coefficient, and true population slope was 2, the normal distribution will be centered around 2). The width (spread) of the distribution is governed by sample size. Larger sample sizes result is narrower distributions (less spread .. with most values around the true value at the center). The distributions also tend to become more symmetric as the sample size increases.\n",
    "\n",
    "#### Sampling Distribution Properties\n",
    " In terms of sampling distribution properties, the properties of sampling distributions for many popular statistics, regardless of complexity\n",
    "- they tend to be normal\n",
    "- they tend to be symmetrical\n",
    "- they tend to be centered at the true value \n",
    "\n",
    "In larger sample sizes means that you have less variability in the estimates. The key point that's coming across here is that __we can estimate the variances of these normal distributions based on only one sample__. This is what enables us to make inference about the larger population while also accounting for sampling variability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Normal Sampling Distributions\n",
    "Not all statistics are going to have normal sampling distributions like we've seen now for __means and correlation coefficients and regression coefficients__. In these cases, more specialized procedures are needed to make population inferences, and we're going to talk about ideas like Bayesian methodology. We will talk more about this later.\n",
    "\n",
    "### How to make population inference from sampling distribution (Making Population Inference Based on Only One Sample)\n",
    "We've talked a lot about sampling distributions now. Now we want to take the next step and say based on just one sample how would we ultimately make a population inference. We can estimate features of the sampling distribution based on that one sample, but how do we get from that point to making a population inference?\n",
    "\n",
    "#### General approaches to making population inferences based on estimated features of sampling distributions.\n",
    "We're going to talk about two general approaches\n",
    "1. First approach: estimate a confidence interval for the parameters of interest in the population. \n",
    "2. Second approach: testing hypotheses about the parameters of interest.\n",
    "\n",
    "#### Parameter of interest\n",
    "Some examples of what we mean by a parameter of interest, and this is really the ultimate product of our data analysis. Are we interested in estimating a mean or interested in estimating a proportion or regression coefficient, an odds ratio. There are many statistics out there that we could be using to answer research questions. So, that's what we're referring to when we talk about a parameter of interest, something that describes a feature of the population, whether it's a relationship or an individual descriptive statistic, this is what we're referring to by parameter of interests. \n",
    "\n",
    "#### Key Assumption\n",
    "Key Assumption: __Sampling distribution is normal__\n",
    "\n",
    "A key assumption in making inference is again, this notion of the __normality of the sampling distribution__. So, the approaches that we're going to be talking about, forming confidence intervals and testing hypotheses. These assume that the __sampling distributions for these estimates are approximately normal__. We talked about this in previous lectures, that under different size samples, these distributions will tend to become less variable. We get more precise estimation, but for a variety of relatively modest sample sizes, the distributions will always tend to look normal, based on the central limit theorem. So, again, this idea, this assumption that the sampling distributions are normal is often met if the sample sizes are large enough. Remember, when we talk about a sampling distribution, we're talking about all possible values of an estimate of a parameter of interest across hypothetical repeated samples. So, our inference is going to be driven by what that sampling distribution looks like. \n",
    "\n",
    "#### What if sampling distribution is not normal\n",
    "For some statistics, the sampling distribution is not normal. For those we cannot apply our techniques of confidence intervals or hypothesis testing. For these, we have alternative inferential approaches that we will discuss later.\n",
    "\n",
    "For now, since a large number of very common statistics follow a normal sampling distribution, we will go ahead with the two approaches (confidence intervals and hypothesis testing).\n",
    "\n",
    "### Steps of doing Statistical Inference (for normal sampling distribution)\n",
    "Excellent video here: 'data/Making Population Inference Based on Only One Sample.webm'\n",
    "#### Step 1: Compute a point estimate\n",
    "Step one is to compute what we call a point estimate. This is an estimate of one parameter of interests for the overall population. \n",
    "\n",
    "So, we start by computing an __unbiased point estimate__ of the parameter of interest. \n",
    "\n",
    "What do we mean by an __unbiased point estimate__? \n",
    "\n",
    "__What this means is that the average of all the possible values for that point estimate again, across hypothetical repeated samples is equal to the true parameter value. This average is also known as the expected value of that point estimate__. In other words, unbiased point estimate is the average of all the possible values for that point estimate across hypothetical repeated samples. In contrast, a biased point estimate can be, for example from an non-random sampling. Biased point estimate is not equal to the true value(a.k.a. expected value, a.k.a. unbiased point estimate). So, what that means is that, with bias sampling, on average, the estimate that we're computing will be far away from the true population parameter. __So, an unbiased point estimate means that the sampling distribution is centered at the true population parameter that we're interested in__.\n",
    "\n",
    "What is the value of that point estimate that we would expect to see across hypothetical repeated samples? \n",
    "\n",
    "__When we say that an estimate is unbiased, that means that the average of all these possible values across hypothetical repeated samples is equal to the true parameter value__.\n",
    "\n",
    " So, we tried to design samples that will produce data that on average, when we draw samples repeatedly, those estimates that we compute will be equal to the true population parameter. This is why it's so important to design samples that will give us these types of unbiased estimates. So, a key idea here. We want the estimate that we're computing, to be unbiased with respect to the sample design. So, given the sample design that was used, we want to compute an estimate of a population parameter of interest that is unbiased when we account for features of the sample design that was employed to collect the data. If cases have unequal probabilities of selection for example, that leads to the need to use weights in computing our finite population parameter estimate. We need those weights to compute an unbiased point estimate. If we fail to use weights when sample designs have unequal probabilities of selection, say I have a much higher probability of being selected than one of my colleagues, I need those weights to get back to an unbiased picture of the overall population. A failure to use weights in estimation is going to shift our sampling distribution. So, we have our point estimate, we compute an unbiased point estimate, if we need to use weights based on the sample design, we would do so in computing that point estimate.\n",
    " \n",
    " #### Step 2: Compute sampling variance estimate  \n",
    " Now, we need to estimate the sampling variance associated with that point estimate.  What that means is, we want to estimate the variance of that sampling distribution. \n",
    " \n",
    " Note: __sampling distribution of sample variance will be a Chi-Square distribution if the population was normal, if the population was not normal, it is very difficult to define mathematically the exact shape of sampling distribution of sample varience__.\n",
    " \n",
    " Again, we don't have the resources to draw thousands and thousands of samples and compute their variance based on all those different samples. We need to estimate that sampling variance associated with our point estimate based on one sample only. \n",
    " \n",
    " So, much like computing unbiased point estimate of our parameter of interest, we also want to compute an unbiased estimate of the sampling variance for that sample distribution for this particular point estimate. So, we want an unbiased point estimate and we want an unbiased estimate of the sampling variance. \n",
    " \n",
    " So, __an unbiased variance estimate correctly describes the variance of the sampling distribution under the sample design that was used__. So, again, if we plot all the hypothetical values of a possible estimate, you see that little blue arrow in this particular plot. That's the variability that we're trying to estimate based on only one sample. Once we have a good unbiased estimate of variability, meaning the variability that we would expect to see, given the sample design that was used, then we have the tools that we need in order to make inference about the value of that parameter in the overall population. \n",
    " \n",
    " __Standard Error__: The square root of the variance that we're trying to estimate, that's what we referred to as the __standard error of the point estimates__. \n",
    " \n",
    " So, when we estimate the variance of the sampling distribution to make inference, we generally rely on what's called the standard error of the point estimate. What that really means is on average, how far away are the different estimates that we compute in the samples from the truth? We get an estimate of that standard error by taking the square root of the variance. So, another thing that you can conclude from this result is that, the standard error is really describing the standard deviation of the sampling distribution. So, the __standard error is the standard deviation of the sampling distribution of estimates__. We use this standard error to make inference about the overall population parameter. \n",
    "\n",
    "So, __to form a confidence interval__:\n",
    "- __Best Estimate__ $\\pm$ __Margin of Error__\n",
    "- __Best Estimate__ = Unbiased point estimate\n",
    "- __Margin of Error__ = \"a few\" Estimated Standard Errors \n",
    "- \"__a few__\" = multiplier from appropriate distribution based on desired confidence level and sample design. It is commonly 2 standard errors for 95% confidence interval and 0.05 Significance\n",
    "\n",
    "The key idea is that for a 95 percent confidence interval, we would expect that 95 percent of the intervals computed this way are going to cover that true population value across these repeated samples. \n",
    "\n",
    "#### Interval = range of reasonable values for the parameter\n",
    "\n",
    "If the hypothesized value for parameter lies <u>outside</u> confidence interval, we don't have evidence to support that value at corresponding significance level\n",
    "\n",
    "The caution here is that, it's important to get all three pieces bright to make correct population inferences. \n",
    "- If the best estimate in forming these intervals is not in fact an unbiased point estimate\n",
    "- If the margin of error does not use the correct multiplier (we were using two in general for a 95 percent confidence interval)\n",
    "- If margin of error does not use an unbiased estimate of the standard error\n",
    "\n",
    "Then the confidence interval will not have the advertised coverage of the true population parameter. So, even though we might call it a 95 percent confidence interval, maybe it only covers the true parameter 90 percent of the time, or 80 percent of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing\n",
    "__Hypothesis__: Could the value of the parameter be _____ ? \n",
    "\n",
    "That blank is the hypothesized or __null value__ \n",
    "\n",
    "We really answer the question. __Is the point estimate for the parameter close to this null value, or is it really far away?__ \n",
    "\n",
    "\n",
    "### Test statistic = $\\frac{estimate \\ - \\ null \\ value}{standard \\ error}$\n",
    "\n",
    "That gives us an idea of how far away our estimate is from that particular null hypothesis value. How much evidence we have against that particular null value. If the null is true, then we can look at the probability of seeing a test statistic this extreme or even more extreme. If the probability of seeing a test statistic that large under the null hypothesis is small, we would reject that null hypothesis. \n",
    "\n",
    "So, an important reminder about these two inferential procedures (Confidence intervals and hypothesis testing), all of these inferential procedures are valid __if probability sampling was used__. All the techniques that we just described rely on the idea of a probability sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preventing Bad/Biased Samples\n",
    "\n",
    "Many so-called “standard” statistical analyses that are presented and discussed in introductory statistics courses make the assumption that the data of interest are __independent and identically distributed (or \"i.i.d.\") observations__. As discussed in the lectures earlier this week, __simple random sampling (SRS)__ is the closest probability sampling analog to i.i.d., in that the sampling mechanism used to generate the observations will produce independent and identically distributed observations. While this type of sampling will produce samples with this nice “i.i.d.” statistical property, facilitating “standard” statistical analyses, SRS is seldom used when sampling from real populations. One of the reasons for this is that SRS, while producing estimates that are unbiased in nature (which recall means that the estimates based on hypothetical repeated samples using SRS will have a mean equal to the true population mean), has the potential to generate “bad” samples with substantial sampling error (where an estimate based on the sample is quite different from the population parameter of interest).\n",
    "\n",
    "Consider, for example, a national sample of 1,000 cell phone numbers selected using SRS. While in expectation any one given sample will include a representative random sampling of numbers from area codes across the nation, __all possible random samples using SRS are equally likely__. What this means is that a simple random sample of cell phone numbers that only includes area codes from Florida is just as likely as a simple random sample of numbers that includes a representative selection across the states. Ideally, we would like to use design strategies to reduce the chances of such a “bad sample” occurring, especially if our variable of interest tends to take on very different values in the state of Florida! The major statistical problem with the simple random “Florida” sample is that any estimate that we compute after collecting data from the sample will likely be very different from the true population parameter that we are trying to estimate (especially if the variable of interest tends to take on very different values in Florida relative to the rest of the nation). Because the probability of selecting these extreme samples is equal to the probability of selecting more representative samples, the sampling distribution for simple random samples can tend to be quite variable.\n",
    "\n",
    "A very common sampling technique used to minimize the sampling variance that can arise from these so-called “bad samples” in SRS is __stratification__. You’ve already been introduced to stratification in an earlier lecture. When we conduct stratified sampling, we first allocate portions of our sample to all possible divisions (or “strata”) of the population of interest (e.g., states). This ensures that some sample will be selected from all of these possible divisions, and that the overall sample will therefore be representative of the target population. For example, using a technique known as proportionate allocation, suppose that we knew that 55% of students enrolled in a particular college were females, and 45% were males. If we wanted to draw a sample of 1,000 students from this college, we would randomly selected 550 females from a list of all females enrolled, and 450 males from a list of all males enrolled. This ensures that our entire sample of size 1,000 won’t include only females!\n",
    "\n",
    "Consider our earlier gym example as well, and the web app that allowed us to visualize sampling distributions. If we only draw our sample from one stratum of the overall population (e.g., gym goers), and the units in that stratum tend to have values on a variable of interest that differ from the values for the variable in other strata, then the estimates that we compute based on that sample will be biased, and will not represent the overall target population. This is an example of __selection bias__; on average, estimates computed from repeated samples of gym goers will not be equal to the true population parameter of interest. Stratified sampling ensures that we would select a sample of gym goers and a sample of non-gym goers, increasing the representativeness of our sample and potentially reducing bias.\n",
    "\n",
    "Another nice property of stratified sampling is that it shrinks the variance of sampling distributions. In SRS, all of the variance within strata and between strata in terms of the variable of interest contributes to the overall sampling variance. In stratified sampling, when we allocate a certain number of sampled units to be selected from each stratum, we remove the between-stratum variance from the overall sampling variance! This is because every hypothetical repeated sample would use the same stratified design, and the same allocation; assuming reasonable response rates, we will have representation from each of the strata where we allocated a portion of the sample. There is no uncertainty in whether we will have sampled units from a particular stratum, and there is nothing random about the allocations; these are fixed by design! The only uncertainty arises from the random sampling that occurs within strata from one hypothetical sample to another. Each sample will always feature random selections from the same strata; what happens within the strata will change from sample to sample.\n",
    "\n",
    "We will revisit the idea of stratified sampling in an upcoming lecture, but you will often hear sampling statisticians say __\"when in doubt, stratify.\"__ We can use this technique to prevent bad samples, and decrease the variance of our sampling distributions.\n",
    "\n",
    "### Bad Samples Arising from Nonresponse\n",
    "When analyzing data, we always have to think carefully about the process used to ultimately produce the data that we are analyzing. We may dedicate substantial resources to a carefully designed stratified sample of some population that will produce unbiased estimates by design; _however, there is no guarantee that every unit sampled will agree to provide data_. If a sampled unit refuses to provide data after being sampled, this situation is known as __unit nonresponse__. Unit nonresponse can have a particularly negative impact on the quality of a given sample when the units that ultimately agree to provide data differ significantly from the units that do not agree to provide data on the variables of interest.\n",
    "\n",
    "For example, suppose that people with lower income tend to respond to a survey of a nationally representative sample of individuals at higher rates than people with higher income. Because the resulting sample of respondents to the survey request tends to feature people with lower income, any estimates related to income (which will always be computed using data from the respondents, or the units that agree to provide data!) will be subject to another form of __selection bias__, namely __nonresponse bias__. In short, nonresponse bias occurs when there is a tendency for the units in a sample that agree to provide data to be systematically different from the units in the sample that do not provide data (in terms of the variable of interest). This type of bias can also occur for estimates based on specific variables, when sampled units may agree to provide data in general, but not on specific variables. For instance, a survey respondent may agree to participate in the survey, but refuse to share their income. This type of nonresponse is known as __item nonresponse__.\n",
    "\n",
    "Whereas stratified sampling is a design tool that can be used to reduce selection bias from a sampling perspective, the selection bias introduced by unit or item nonresponse can either be addressed during the data collection process or via post-survey adjustments to the estimates based on a respondent sample. For example, sampled units reluctant to provide data may be offered additional incentives for their participation, or offered different methods for providing their data (e.g., over the web, rather than speaking to an interviewer). Such units may also receive additional effort from a data collection organization (e.g., more follow-up contact attempts). After the survey is over, if there is still evidence that the respondent sample somehow differs systematically from the full sample, respondents who had a lower probability of responding may receive larger weight in the overall analysis. Item nonresponse may be addressed via statistical models used to predict the missing values as a function of other observed data. There are all tools designed to reduce the type of selection bias that can arise from nonresponse.\n",
    "\n",
    "It is essential that anyone computing estimates based on samples of populations carefully evaluate the steps that were taken to minimize the potential biases arising from these “bad” samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Samples\n",
    "A complex sample is any probability sample where the sample design involves more than just simple random sampling.\n",
    "\n",
    "## Features of Complex Samples:\n",
    "### Stratification\n",
    "__Stratification__ is defined as the allocation of an overall sample to different strata or mutually exclusive divisions of the larger population.\n",
    "\n",
    "- When we create sampling strata, we often allocate some amount of our sample to come from each of these different regions. There are several different potential allocation schemes in this case for stratification. __Our aim is to minimize sampling variance for particular variables given fixed costs__.\n",
    "- Stratification will eliminate between-stratum variance in means (or totals) on variable from the sampling variance. In other words, this removes the variability between the strata in our variables of interest from the sampling variability. We're always going to have some sample from each of those different strata. If we don't stratify a sample, we run the risk of selecting random samples where we don't have representation from some of those strata. And this stratification ensures that were always selecting samples from these different strata. We've removed the between stratum variability from our overall sampling variants. Because we always have sample coming from the different strata across the hypothetical repeated samples. So this is a good thing in terms of our overall estimates.\n",
    "- It's very important to account for this kind of stratification in our analysis. Otherwise, our sampling variance may seem like it's too large. Our estimates of the sampling variance, remember we talked about making inference based on a probability sample, our estimates of those standard errors may be larger that they should be. In other words, there are going to be biased estimates of the sampling variance if stratification was actually used in selecting a sample and we don't account for that when we actually compute the standard errors. And what this means is that our inferences would be too conservative, our confidence intervals would be too wide, because we have more sampling variability than we really should if stratification was used. So it's very important to account for stratified sampling when we actually analyze data.\n",
    "\n",
    "### Clustering\n",
    "Stratification and Clustering often get confused. Stratification is much more strict. Stratification works when it is feasible to easily divide population in mutually exclusive groups. Like cifizens of each state in US. Here we can make state as strata. They are mutually exclusive and we can easily weight our random samples to take a guaranteed fraction from each state. But it is not always possible to divide the entire population into strata like this. Like all people who have had none, one or more heart strokes. Here it is not as obvious to divide the entire population into strata of poeple had zero or more strokes. So, we divide the population into clusters, like states. Then further clusters in each cluster like counties, then particular region within a county, then a street within that county. We then do SRS at each cluster stage. Like choose one state at random. Then choose one county at random from that state. Then choose one region at random from that county. Then choose one street at random from that county. Then finally, we poll __everybody__ in that street for how many strokes they had. This is clustering. Its more convenient and less expensive than stratification. In other words, this is a great tool for selecting a sample at random but minimizing the cost of data collection. \n",
    "\n",
    "- Clustering tends to increase sampling variance of estimates because units within the same cluster have similar (correlated) Values on variables of interest i.e. we don't measure unique info.\n",
    "- It is very __important__ to account for cluster sampling in analysis, else inferences are too _liberal_, confidence intervals too _narrow_.\n",
    "\n",
    "So if we draw very different clusters in any given sample, there could be a lot more variability in our estimates of sampling variance and standard errors. So the spread of that sampling distribution tends to get larger when we draw a cluster sample. So we had this kind of trade-off. Clustering reduces cost on one hand, but we tend to have more sampling variance on the other hand. Our estimates have more variability. And again, this happens because units coming from the same cluster tend to have similar or correlated values on the variables of interest. We're not measuring unique information when we draw the cluster sampling. So we do it to save costs, but we don't pick up on as much unique information as we could if we weren't drawing a cluster sample. And we have this risk of inflated sampling variability if the clusters tend to be very different from each other.\n",
    "\n",
    "### Weighting\n",
    "A third very important feature of complex sampling is weighting (others being stratification and clustering). So remember, complex samples are probability samples. But if these complex samples are defined by multiple stages of cluster sampling within strata, like we talked about counties, then area segments, then households, if that kind of sampling design is used within sampling strata. Or there are certain subgroups of the population that are sampled at higher rates, this idea of oversampling from particular population subgroups, we end up with unequal probabilities of selection for different population units. So the probability of being included in the probability sample could be very different for different people. Depending on their characteristics, depending on where they were sampled from, and so forth. And if there are in fact unequal probabilities of selection for different people in the overall population, we need to account for these unequal probabilities to make unbiased population inferences. Otherwise, we're going to get a biased picture of the population. Because maybe certain types of people were oversampled relative to how often they appear in the population. Or certain types of geographic areas had larger samples selected than, again, would be in the overall population picture. So the sample that we get following these kinds of designs may not be an exact reflection of the overall population. And the weights for accounting for these unequal probabilities of selection give us a tool for making sure that the sample looks like the overall population, accounting for these unequal probabilities.\n",
    "\n",
    "Weights can also be adjusted for different possibilities of responding to a given survey request and different subgroups. So suppose that my probability of selection was 1 out of a 100, like we just discussed, but I belong to a population subgroup, say Caucasian males, where only 50% of Caucasian males actually responded to the survey request. I need to account for this differential probability of responding to the survey request in adjusting my overall weight. So my adjusted weight in this context would be my initial sampling weight, 1/0.01 or 1/1 over 100. I would multiply that sampling weight by 1 divided by 0.5, which is my probability of responding to the survey request, depending on the subgroup that I belong to. So not only do I have to speak for myself and 99 other people based on the sample design, according to that sample design, only one out of every two people actually responded who are like me. So I have to speak for 99 other people to begin with, but then I also have to speak for all the other people who didn't respond to the actual survey request. And I need this additional adjustment to the overall weight that would be used in analysis. So given the 50% response rate in my particular subgroup, white Caucasians, that means that I now have to speak for myself and 199 other people. All those additional people who were sampled like me, where only 50% of them responded. So my weight goes from being 100 to actually being 200. I have to speak for 199 other people who didn't respond and belong to my particular subgroup. So weights are often adjusted for non-response to account for that differential non-response across different subgroups. because you'll never see a survey where there's 100% response rates, and response rates tend to be going down over time. So we need to adjust for that. And one of the ways that we do that is by adjusting these survey weights for the non-response.\n",
    "\n",
    "The __drawback__ of using weights in analysis is that much like cluster sampling, highly variable adjusted survey weights tend to increase the sampling variance of weighted estimates. So there's a lot of variability in our weights, that means there's a lot of uncertainty in terms of who we're actually sampling and who they represent. And if you think about this across repeated samples, if there's a lot of variability in the weights, that can lead to a lot of variability in our estimates across the repeated samples. So highly variable survey weights will tend to increase the sampling variance of our weighted estimates, even if the weights produce unbiased point estimates.\n",
    "\n",
    "## Summary\n",
    " So, putting all those ideas together, we can think about what are called __design effects__, and that's the effect of a complex sample design. On the overall sampling variance that we're interested in estimating. So thinking about those three key complex sampling features that we talked about, __stratification reduces sampling variance__. It gives us more precise estimates, because we're removing that between strata variability from the overall sampling variability. Every hypothetical sample will come from the same strata of the population.\n",
    "\n",
    "__Cluster sampling__ and __weighting__, on the other hand, they tend to __increase sampling variance__.\n",
    "\n",
    "So the net multiplicative effect of all of these three different complex sample design features on the standard error is what we refer to as a design effect. \n",
    "\n",
    "A failure to identify those features of complex samples (weighting, stratification and clustering) when performing data analysis is what's often referred to as __analytic error__. And many individuals who perform secondary analyses of survey data sets don't take these sampling features into account, unfortunately. This, again, like we've been discussing, can lead to very biased inferences based on the survey data. If it's available, we need to at least consider them in computing our point estimates. If stratification and cluster sampling was used to select the sample, we need to account for those features in estimating the sampling variance. We need to come up with a good estimate of what that sampling variance would look like if we did, in fact, draw hypothetical repeated samples using that design. So failure to account for these features is known as analytic error, and many secondary analysts that survey data are not aware of these issues. This is where it is very important to be aware of where your data comes from, and what type of sample design was used before you start to analyze the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
