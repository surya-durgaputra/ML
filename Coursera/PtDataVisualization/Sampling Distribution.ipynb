{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.psychologyinaction.org/psychology-in-action-1/2016/08/13/what-is-a-sampling-distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non sample method is that of Population census. Here every single unit is measured in the target population. This is easier for smaller target populations but incredibly expensive, difficult or sometimes impossible for larger populations.\n",
    "\n",
    "Hence we depend on population sampling. \n",
    "\n",
    "__Two types of Sampling__\n",
    "### Non - Probability Sampling:\n",
    "- Generally does __not__ involve random selection\n",
    "- Probabilities of selection <u>can't be determined</u> for population units since we have no control over who gets sampled. It is often opportunistic in some way like in examples below and hence can be heavily biased.\n",
    "- Examples:\n",
    "    + __opt in web surveys__ (take whoever is interested in taking that survey. We are not selecting people at random from some well defined list or sampling frame. Its whoever wants to volunteer in that survey and as a result we can't determine the probabilities of selection)  \n",
    "    + __quota sampling__ you try to recruit as many people as you can who fit certain subgroup definition (like African american males or older african american males) until you hit some targets, some number of individuals that you wish to measure. In many cases, researchers try to sample as many inidividuals as they can, not according to any probability scheme, but just on basis of whoever is available and just try to hit their quotas. This is also non-probability sampling as we can't write down the probabilities of a target being selected. We are just trying to meet targets.\n",
    "    + __snowball sampling__ This is a network sampling where a sampled individual refers a friend, who then refers a friend and so on. In this case again, friends are recruiting friends. We don't really have any control over who they recruit or the probabilities with which they are going to recruit these individuals. So snowball sampling is a convenient tool for sampling but as researchers, we have no control over probabilities of selection of those samples.\n",
    "    + __convenience sampling__ Walk down the street and talk to people who are available. Talk to friends or co-workers. Again, no probabilities of selection involved. You are just trying to collect data from individuals who are convenient and in close proximity to you. Again we can't apply probabilities of selection for these individuals and that prevents us from making representative statements about the larger population. It can heavily biased. \n",
    "\n",
    "### Probability Sampling:\n",
    "- Construct list of all units in population = __Sampling Frame__\n",
    "- Determine __probability of selection__ for every unit on the list (known and non-zero)\n",
    "- __Select units from list at random__, with the sampling rates for different subgroups determined by probabilities of selection\n",
    "- Attempt to __measure__ randomly selected units\n",
    "\n",
    "__Why prefer Probability Sampling:__ The known probabilities of selection for all units allow us to make unbiased statements about both population features (the stuff that we are trying to estimate when we analyze the data) and the uncertainity in survey estimates. So in addition to saying that what is the average income of the people, we would also like to say how uncertain we are about those estimates because we are not measuring everyone in the population, we are measuring a sample of individuals. And we want to make an estimate of how uncertain we are with the estimate we are computing.\n",
    "\n",
    "Random selection of population units __protects us against bias from the sample selection mechanism__. \n",
    "\n",
    "__Probability sampling__ allows us to make population __inferences__ about larger populations, based on __sampling distributions__. \n",
    "\n",
    "If we draw a sample from a population, and calculate our estimes from that sample, and we do this process repeatedly, over time, a distribution of estimates emerges. This is called sampling distribution. In reality, we often make only one sample and if the sample drawing was carefully and scientifically designed, we can do representative estimates about larger population from this sample. The big idea is that with careful design, probability samples yield __representative, realistic, random__ samples from larger populations; such samples have statistical properties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Sampling types:\n",
    "#### Simple Random Sampling (SRS):\n",
    "- Start with know list of N population units, and randomly select n units from the list. So n is the sample size.\n",
    "- Every unit has __equal probability of selection__ = n/N\n",
    "- All possible samples of size n are equally likely.\n",
    "- Estimates of means, proportions and totals based on SRS are __unbiased__ (equal to the population values on average!). Note that for each sample of size n drawn, there might be variability in the estimates, but on average and over a large number of samples, these estimates will be the same as true population estimates.\n",
    "- SRS can be __with replacement__ or __without replacement__\n",
    "    + for both: probability of selection for each unit is still n/N\n",
    "- SRS, though simple, is rarely used in practise. Collecting data from n randomly sampled units in a very large population can be prohibitively expensive. __Exception is when there is relatively cheap data collection based on well-defined population lists or we have a collection of file records where we can literally pull records out of file cabinets.__\n",
    "- SRS is generally done when the populations are smaller and its easier and simpler to collect the data.\n",
    "- SRS connection to __i.i.d. Data__:\n",
    "    + recall that i.i.d. observations are __independent__ and __identically distributed__\n",
    "    + SRS will generate i.i.d. data for a given variable, in theory\n",
    "        - All randomly sampled units will yield observations that are independent( not correlated to eachother) and identically distributed (representative of some larger population of value, again, in theory)\n",
    "\n",
    "Example of SRS:\n",
    "We have a list of 2500 emails received. We need to find the average response time for each email. Here, we cannot just take all 2500 emails because due to infrastructure limitations, to find the response times, we will need to go through each of the email manually. So we do SRS and our sample size is 100.\n",
    "So our sampling design for SRS is: We number emails 1 to 2500 and randomly select 100 using a random number generator.\n",
    "- __Every email has known probability of selection__ = 100/2500\n",
    "- Produces __random, representative sample__ of 100 emails (in theory)\n",
    "- __Estimated mean response time__ will be an __unbiased__ estimate of population mean\n",
    "    + this is a feature of probability sampling - __On average if we were to draw many, many samples of size 100 and compute the mean of each of those, the average of those means will be equal to the true population value.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex Probability Sampling\n",
    "- With larger populations, __complex samples__ often selected, where each sampled unit has known probability of selection. __Complex = anything more complicated than SRS!__\n",
    "- With complex probability sampling, we use very specific features of sampling design that enable us to save on cost and make our samples more efficient.\n",
    "- __Complex samples have certain key features__:\n",
    "    + Population divided into different __strata__, and part of sample is allocated to each __stratum__; this ensures sample representation from each stratum, and reduces variance of survey estimates (__stratification__)\n",
    "    + __Clusters__ of population units (e.g. counties) are randomly sampled first (with known probability) within strata, to save costs of data collection (collect data from cases close to each other geographically). We often use group, strata and cluster interchangeably. In this case, cluster within a stratum is just a multi-level cluster.\n",
    "    + __Units randomly sampled within clusters__, according to some probability of selection and measured\n",
    "- **__So a unit's probability of selection is determined by__**:\n",
    "    + Number of clusters sampled from each stratum\n",
    "    + Total number of clusters in population in each stratum\n",
    "    + Number of units ultimately sampled from each cluster\n",
    "    + Total number of units in population in each cluster\n",
    "\n",
    "Example of finding a unit's probability of selection:\n",
    "- Select **__a__** out of __A__ clusters at random in a given stratum\n",
    "- then select **__b__** out of __B__ units at random from within a selected cluster\n",
    "\n",
    "Probability of selection = $(\\frac{a}{A})(\\frac{b}{B})$\n",
    "The stratum here can be Midwest US. Clusters can be all the counties in the midwest (each county being a cluster). So people designing constraints will decide what a and b will be in accordance to the cost constraints.\n",
    "\n",
    "Example: NHANES\n",
    "- Divide U.S. into different regions based on geography and population density. We refer to these divisions as __strata__ And again, by allocating samples to each of these stratum, we ensure some representation, thus minimizing the risk of a bad simple random sample where when selecting samples totally at random, they just happened to all come from one region. This this approach ensures __increased representation__. \n",
    "- Allocate some number of counties / groups of counties to be sampled from each stratum (These are called __clusters__. We randomly sample clusters again to save costs. This way we can randomly sample households that are within a specific geographic area rather than pure SRS where the households can be from anywhere in US)\n",
    "- Sample certain socio-demographic subgroups of individuals at higher rates within counties. This is called __oversampling__. So maybe, a given project has a certain target sample size for particular subgroups of individuals. We might sample these separate subgroups at higher rates within those counties (or cluster) when we are randomly selecting households. What this leads to is different probabilities of selection for different types of individuals depending on the goals of a project. That oversampling means that different people will have different probabilities of being selected, different rates of selection at that second stage, and that's okay. We still have a probability design and we can make use of those probabilities to ultimately make representative statements about the larger population.\n",
    "\n",
    "__Stage 1__: We have entire US. We can subdivide the US into regions and then sample counties.\n",
    "\n",
    "__Stage 2__: For a county from stage 1, we then randomly choose some areas within a county. This again is a cost saving measure. This is already a multi-stage cluster (stage 1 then stage 2)\n",
    "\n",
    "__Stage 3__: For a particular area within a county from stage 2, we get a list of all the households and do a simple random selection of households.\n",
    "\n",
    "__Stage 4__: At his stage, a field member can actually visit the households from stage 3 and randomly choose a subject within that particular household (like if there are 3 people in a household, randomly choose one). We then take all the measurements from this individual as seen in NHANES dataset.\n",
    "\n",
    "At all four of these stages, we know what the probabilities of selection are. And we maintain those probabilities of selection throughout the entire design. Ultimately we can can compute the probability of being include for every single individual that we might randomly sample.\n",
    "\n",
    "In this type of design, the inverse of a person's probability of selection is then their __sampling weight__. So if my probability is 1/100, my sampling weight is 100. In other words, I represent myself and __99 others__ in the population!\n",
    "\n",
    "This sampling weight, which is a function of probability of selection, is used in the actual data analysis to compute representative population estimates. So that probability of selection plays a direct role in the computation of estimates based on complex samples.\n",
    "\n",
    "So, summarizing, \n",
    "- those weights get used to compute __unbiased estimates__ of population quantities (e.g. mean BMI), accounting for different probabilities of selection.  \n",
    "- Probabilities of selection play a __direct and essential role__ in computation of unbiased population estimates!\n",
    "\n",
    "So __WHY PROBABILITY SAMPLING__:\n",
    "- Having __known, non-zero probability of selection__ for each unit in a population and __subsequent random sampling__ ensures all units will have a chance of being sampled\n",
    "- __Probability sampling__ allows us to compute __unbiased estimates__ (using those sample wieghts), and also estimate features of __sampling distribution__ of estimates that we would see if many of the same types of probability samples were selected. We can actually simulate what that sampling distribution would look like based on selecting only one sample. This is the beauty of probability sampling. We dont have to draw samples over and over again, to get a sense of what that distribution of estimates might look like.\n",
    "\n",
    "Thus, Probability sampling provides a __statistical basis for making inferences__ about certain quantities in larger populations. We can make inferences about the entire population on the basis of just one sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non Probability Samples\n",
    "\n",
    "Features of __non-probability__ samples:\n",
    "- Probabilities of selection __can't be determined__ for sampled units.\n",
    "- __No random selection__ of individual units. So we dont control the random selection mechanism that ultimately yields the sample in a Non-probability sample.\n",
    "- Sample can be divided into groups(strata) or clusters, but __clusters not randomly sampled__ in earlier stages.\n",
    "- Data collection often very cheap relative to probability sampling. This is the big advantage of non-probability samples.\n",
    "\n",
    "Example of Non-probability Sampling:\n",
    "- Study of volunteers (e.g. clinical trials):\n",
    "    + You would often see a flyer or posting online like \"Do you suffer from a disease XXX. If yes, call XXX-XXXX (phone number) and become part of the study\". And then someone calls that number to become part of this study. \n",
    "    + So here, there are no sampling frames or lists. Researchers are just looking for volunteers to join their study. They have no control over who volunteers to join the study.\n",
    "- __Opt-in__/ Intercept web surveys:\n",
    "    + So when you are on a website and you see an invitation to come and complete a survey or you see an opinion survey on a particular website and you decide to join this particular survey.\n",
    "    + Again, there is no probability of selection. There is no random selection. The people trying to collect the data are just looking for volunteers to ultimately join the survey.\n",
    "- __Snowball__ samples:\n",
    "    + This is where the sample grows by people referring others for data collection. But there are no probabilities of selection that govern who participates in the survey.\n",
    "- __Convenience__ samples:\n",
    "    + Students or faculty conduct a study and select samples from their class.\n",
    "- __Quota__ samples:\n",
    "    + You have certain targets that are needed to be met. Like recruuit 1000 males and 1000 females in any way.\n",
    "    \n",
    " \n",
    "### Common feature of Non Probability samples \n",
    " Probabilities of selection <u>cannot be determined</u> a priori (or before you begin the study)! . This is the crucial difference between probability sampling and non probability sampling.\n",
    " \n",
    "Hence in a non probability sample:\n",
    "- there is __no statistical basis for making inference__ about larger population from which sample was selected.\n",
    "- There is a strong risk of __sampling bias__ because samples units are not selected at random.\n",
    "- Sampled units __not generally representative__ of a larger target population of interest.\n",
    "\n",
    "All these make it difficult to make an unbiased estimate of the overall population.\n",
    "\n",
    "But all is not lost. It is still possible tosay something about larger populations using datasets made of non-probability samples. There are two possible approaches:\n",
    "- __Pseudo-Randomization__\n",
    "    + With some transformation of data, we can treat Non-probability sample like a probability sample.\n",
    "- __Calibration__\n",
    "    + You weight your non probability sample to look more like the population that you're interested in. So this involves weighting and other model based adjustments.\n",
    "\n",
    "These approaches can transform a non-prabability sampled data into a dataset that looks as if it came from a probability sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-Randomization Approach\n",
    "- We __combine non-probability sample with a probability sample__ (i.e. \"stack\" data sets together). We find groups of samples in the probability sample that are similar to those in the non-probability samples and literally stack them together.\n",
    "- __Estimate probability of being included in non-probability non-probability sample__ as a function of auxiliary information available in both samples\n",
    "- __Treat estimated probabilities of selection as \"known\"__ for non-probability sample, use probability sampling methods for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Approach\n",
    "- __Compute weights for responding units__ in non-probability sample that allow weighted samples to mirrow a known population.\n",
    "\n",
    "For example:\n",
    "__Non probability sample__:70% female, 50% male\n",
    "__Population__: 50% female, 50% male\n",
    "\n",
    "So, down-weight females and Up-weight males\n",
    "\n",
    "__Limitation of Calibration Approach__: If weighting factor not related to variable(s) of interest, it will not reduce possible sampling bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Example: Non-probability sample\n",
    "\n",
    "API to extract info from several hundred thousand tweets and indicator of support for President Trump computed\n",
    "- __Probability__ of a tweet being selected __cannot be determined__\n",
    "    + We just grabbed all these tweets where there was a mention of President Trump. In some sense this is a convenience sample of tweets and we can't determine the probability of one tweet being selected using this mechanism. We don't have a frame of all possible tweets. Tweets don't have probabilities of selection. We don't randomly select them. We just take whatever is available that indicated some mention of President Trump.\n",
    "    + __Twitter users are not a random sample__ of a larger population. So, people who joined twitter are interested in expressing their ideas or opinions about particular topics and people can choose to join twitter.\n",
    "    + So, __Lots of data__, but:\n",
    "        - high potential for sampling bias\n",
    "            + we have lots of data (hundreds of thousands of tweets with President Trump in them) but due to lack of probabilities of selection, no random sampling of tweets, there is a very high chance of sampling bias.\n",
    "        - lack of representation: may only capture people with strong opinions! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we will focus on __Sampling distributions and sampling variance__. We will see how to estimate features of these distributions <u>based on only one probability sample</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Sampling Distribution\n",
    "\n",
    "Say we sample all the people in a country of their weight (a census survey). We then plot all the values of weight we got on a chart (distplot in seaborn with weights on x axis and freq on y axis). This is called a distribution and it shows the distribution of weights of citizens of that country.\n",
    "These kinds of variables (like weights of citizens of a country) often follow a distribution called a Normal Distribution.\n",
    "\n",
    "So, a distribution is a __set of values on a variable of interest__.\n",
    "\n",
    "In a __sampling distribution__, we take a sample from a larger population. We __assume__ that values on variable of interest would follow certain distribution <u>if we could measure entire population</u>. \n",
    "\n",
    "Hence, when we select probability samples to make inferential statements about larger populations, we refer to __sampling distribution__.\n",
    "\n",
    "#### <font color=red>It is very important to differentiate a sampling distribution from a distribution of values on a variable of interest.\n",
    "\n",
    "__Sampling Distribution__= distribution of __survey estimates__ we would see __if__ we selected many random samples using __same sampling design__, and __computed an estimate from each__.\n",
    "\n",
    "A sampling distribution is __NOT__ a distribution of values on a variable of interest. It is a distribution of survey estimates. Specifically, ampling distribution is a distribution of __survey estimates__ we would see __if__ we selected many random samples using __same sampling design__, and __computed an estimate from each__.\n",
    "\n",
    "So when we talk about Sampling distribution, we talk about distribution of values of estimates. If we had selected thousands and thousands of hypothetical and repeated random probability samples, all using the exact same probability sampling design, we will get this sampling distribution. If we had the luxury of doing that and we estimated our estimate of interest, maybe its a mean or a proportion from each of those probability samples, a distribution of estimates would arise. Just like we talk about the distribution of values on a single variable of interest, if we were to draw thousands and thousands of probability samples, we would see a distribution of estimates, where any one sample that we draw, we would compute an estimate and then the distribution would be what all of those estimates would look like if we repeated that probability sampling process over and over again.\n",
    "\n",
    "- __Key properties of Sampling Distribution__:\n",
    "    + __Hypothetical__: This distribution is hypothetical. Remember, if we had the luxury of drawing thousands of random probability samples, each one using the same design, and measuring the units in each of those probability samples and then computing an estimate based on all those measures that we collect. We do that over and over and over again and then we plot the distribution of all those estimates that would arise. This is a hypothetical thought exercise. We are not actually going to do that in practise. But the sampling distribution in theory describes what that distribution of estimates would look like if we had the luxury of doing that. But remember that it is a hypothetical exercise and we are trying to estimate the features of that sampling distribution based on one sample only.\n",
    "    + Sampling distribution is also generally very __different in appearance from distribution of values on a single variable of interest__. So we dont want to equate those two ideas.\n",
    "    + With __large enough probability sample size__, sampling distribution of estimates will look like a __normal distribution__, regardless of what estimates are being computed! __Central Limit Theorem:CLT__. This is a very attractive property of probability sampling. The larger the size of our sample and we keep drawing estimates in theory over and over again, the more that sampling distribution is going to look like a normal distribution. That makes it easy to work with these kinds of distributions. This happens because of something called __Central Limit Theorem__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Sampling Variance\n",
    "\n",
    "__Sampling Variance__ = variability in estimates described by the sampling distribution.\n",
    "\n",
    "So, hypothetically, if we were to draw thousands and thousands of those random probability samples and plotted the estimates computed form each of these probability samples, we could describe the variability of that sampling distribution and that's what we mean by __sampling variance__. How variable are the estimates that we are trying to compute across those hypothetical repeated samples.\n",
    "\n",
    "Because we select a __sample__ (and we are not including everyone in the population in that sample), a survey estimate based on a __single sample will not be exactly equal to population quantity of interest__. Remember, with probability sampling, we are selecting cases at random. So we're selecting a sub-sample of all the individuals in the larger population and in any one sample from any of these hypothetical repeated samples, that estimate that we are going to compute is not going to be exactly equal to the population quantity that we're interested in because we're taking a sub-sample of the larger overall population. And that subsample, although we want it to be representative in our probability sampling design, the estimate that we compute will not be exactly equal to the overall population value. This is what is called __Sampling Error__.\n",
    "\n",
    "The fact that we dont measure everybody in a given population and we only measure a sample of individuals in that population, the estimate based on that sample will not be exactly equal to the population value. We just want our estimates across these hypothetical repeated samples to be equal to that population quantity on average. So, if we were to average all of the estimates that we computed from these hypothetical repeated samples, any one of them is not going to be exactly equal to the population quantity that we are interested in. But the average of all those estimates (the __mean__) will be equal to that population quantity. __That's the key property of our sample design that we want and that means we have unbiased estimates__. But the idea that any one sample estimate will not be equal to the true population quantities, say the mean or the proportion we are interested in, that idea is known as __Sampling Error__.  \n",
    "\n",
    "Across hypothetical repeated samples, __these sampling errors will randomly vary__ (some positive, some negative...). In one sub-sample, the mean that we're interested in, that mean might be a little bit lower than the population quantity. Another hypothetical sample, that mean might be a little bit higher. Another hypothetical sample, that mean might be very close to the population quantity. Those estimataes are going to bounce around across these hypothetical repeated samples. Some will be negative, some positive and some will be right on the value that we are trying to estimate. But there's going to be variability in those sampling errors.\n",
    "\n",
    "The variability of those sampling errors describes the variance of the sampling distribution. If we look at that entire distribution of sample estimates, how much they bounce around the true population value that we're trying to estimate, that is the variance of that sampling distribution. And this is what we refer to as sampling variance.\n",
    "\n",
    "So, __if every sample estimate was equal to population quantity of interest__ (for eample, if our probability sample design involved taking a census, where we tried to measure everybody in the population, and we did that over and over and over again, hypothetically of course, then every single estimate will be exactly equal to the true population quantity), there would be __no__ sampling error and __no__ sampling variance.\n",
    "\n",
    "So having zero sampling error and zero sampling variance is a hypothetical ideal and that's what we try to strive for in the case of taking a population census. But as we talked abount earlier, very difficult and very expensive to do that in practise. So instead we take samples.\n",
    "\n",
    "So, with a __larger probability sample size__, sampling more from a given population, in theory there will be less sampling error and sampling errors will be less variable. Now, why is this? Think back to the idea of the census. The closer and closer we get to taking a census, the less sampling error we are going to have. So, the larger a sample size becomes, the closer we get to the population size, the less sampling error we are going to have. Though those error will still bounce around a little bit, but they will be closer to the true population value with a larger sample size. And then, the sampling errors, as a result, are going to be less variable. There's going to be less sampling variance. \n",
    "\n",
    "So this is another critical feature of trying to make inferences about populations. The bigger that your sample size becomes, the less sampling error we are going to have and the less variable our estimates are going to be. __So that Sampling distribution is going to shrink__. Its not going to be as wide as if we had a smaller sample size. \n",
    "\n",
    "__Larger__ Samples = __Less__ sampling variance. __More precise__ estimates, __more confidence__ in inferential statements (__but__ more costly). \n",
    "\n",
    "So, in sampling there is a tradeoff between cost and sampling variance.\n",
    "\n",
    "__Spread__ of sampling distribution becomes __smaller__ as sample size becomes __larger__. This just means that our estimates are less variable.\n",
    "\n",
    "Also, when we use cluster sampling and probability samples, those sampling distributions tend to spread out a little more and become more variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is sampling variance so important for making population inferences based on probability samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have introduced this idea of cost vs variance tradeoff. The larger sample costs more money and shrinks our variance. So we have to deal with these tradeoffs, data collection costing more and getting more precise estimates based on the sampling distribution.\n",
    "\n",
    "In practise, we only have the resources to select __one sample__. In theoretical world, we had this idea of repeated samples. But in the real world, we only have the resources to select one sample and then measure that sample and compute estimates.\n",
    "\n",
    "__Sampling theory__ allows us to __estimate features of sampling distribution__ (including their variance) based on __one sample__. This is one of the wonderful features of probability sampling as it gives us the statistical mechanism to actually estimate the variability of those sampling distributions when we only select one sample. So, we don't have to go out and select thousands and thousands of samples and compute estimates based on each of those samples to determine what those sampling distributions look like. The sampling theory allows us to estimate the features of those sampling distributions by only drawing one sample. This is why probability sampling is so very important for making statements about larger populations. \n",
    "\n",
    "So this is the __Magic__ of probability sampling: We select one probability sample and features of that probability sample design (the probabilities of selection for different individuals, the stratification (if we divided the population into different strata), the cluster sampling (if we selected counties and then neighborhoods and then households).. like that of NHANES), and this gives us all we need to know something about the expected sampling distribution (mainly what the variance of that sampling distribution looks like, and what the mean of that sampling distribution looks like)  .\n",
    "\n",
    "Because we can estimate variance of sampling distribution based on only one sample, we can make inferential statememts about where most estimates based on a particular sample design will fall. And this tells us something about the population. Using a particular probability sampling design, if we can make statements about the variability of our estimates and calculated the overall mean of our estimate, we can make estimates of where all of those estimates based on that probability sample design will tend to fall. And that tells us something about our population.\n",
    "\n",
    "So, __we can make statements about likely values of population parameters__ that account for variability in sampling errors that arises from random probability sampling. So we can make a statement about the overall mean that we're trying to estimate or an overall proportion that we're trying to estimate. Something that describes a feature of the target population. But in addition to computing those overall estimates based on our probability sample, we can also talk about how variable those estimates can be. How much uncertainty was there in the estimate that we're trying to compute. And being able to estimate the variance of the sampling distribution allows us to make a statement about the variability of our estimates that we're computing. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
