{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ = significance level\n",
    "\n",
    "$\\alpha$ represents __Type 1 Error__: _Reject Null when null is true_\n",
    "\n",
    "$\\beta$ represents __Type 2 Error__: _Do not reject Null when alternative is true_\n",
    "\n",
    "If we set the $\\alpha$ at 0.05, then if the probability of our data (p-value) is 0.05 or less, we will reject the null hypothesis.\n",
    "\n",
    "\n",
    "#### <font color=red> Decision Rule: Select $\\alpha$ upfront\n",
    "#### <font color=red> Decision Rule: Reject the Null if p-value $\\leq \\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "## What is a test statistic?\n",
    "## Ans: It's a measure of how far our sample statistic is from our hypothesized population parameter in terms of estimated standard errors. \n",
    "\n",
    "<font color=\"green\">\n",
    "    <h3>4 main steps to a hypothesis test</h3>\n",
    "    <ol>\n",
    "        <li>Stating hypothesis and select significance level $\\alpha$</li>\n",
    "        <li>Checking assumptions</li>\n",
    "        <li>Calculating a test statistic and getting a p-value from the statistic</li>\n",
    "        <li>Drawing conclusions from the p-value</li>\n",
    "    </ol>\n",
    "</font>\n",
    "\n",
    "<font color=\"green\">\n",
    "    Calculating different types of test statistics and getting a p-value from the statistic\n",
    "    \n",
    "Test statistic:\n",
    "\n",
    "- we usually do z-test or t-test where the test statistic comes from a Normal(0,1) or t-distribution. It will give us p-values.\n",
    "- for Difference in proportion tests: we can also do Chi-Square test. This will also give us p-values. They will be similar to those from z-tes or t-test\n",
    "- for Difference in proportion tests:Fisher's Exact test. This will also give us p-values. They will be similar to those from z-test or t-test\n",
    "- for one mean hypothesis test: Wilcoxon Signed Rank Test. This is a non-parametric test. \n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question\n",
    "\n",
    " __In previous years 52 percent of parents believed that electronics and social media was the cause of their teenagers lack of sleep. Do more parents today believe that their teenagers lack of sleep is caused due to electronics and social media?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis test setup\n",
    "\n",
    "So, with this background, we first want to define what our parameter of interest is and our population. So, we have a population of parents with a teenager and that's ages 13 to 18, and our parameter of interest here is going to be __p__. \n",
    "\n",
    "__p__ is the population proportion of parents with a teenager who believe that electronics and social media is the cause of their teen's lack of sleep.\n",
    "\n",
    "our null hypothesis (current belief) is:\n",
    "$$ p_0= 0.52$$\n",
    "\n",
    "our alternate hypothesis is:\n",
    "$$ p_a> 0.52$$\n",
    "<h3><font color=\"green\">Our null hypothesis always contains an =</font></h3>\n",
    "We can also write the same thing as:\n",
    "$$H_0: p=0.52$$\n",
    "$$H_a: p>0.52$$\n",
    "\n",
    "Note, our alternate hypothesis could be less than greater than or not equal to. We chose our alternative will be greater than and the reasoning for that is that the question asked was that \"Do more parents believe\", and so that means we're looking for if p is we're actually going to be greater than 0.52.\n",
    "\n",
    "Finally, we want to set a $\\alpha$ or a significance level which typically this is 0.05. This is basically the cut-off point of when we've found something to be significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"center\"><font color=\"red\">gathering the data and setting up hypothesis</font></h4>\n",
    "Now, we go out and collect our data. \n",
    "\n",
    "So, the Mott poll came back and said that a random sample of 1,018 parents with the teenager was taken and 56 percent said they believe electronics and social media was the cause of their teenager's lack of sleep. \n",
    "\n",
    "Before we go any further with this, we first need to check some assumptions. \n",
    "- the first assumption we need to check is we need a random sample of parents\n",
    "- the second one that will be checking is if our sample size is large enough\n",
    "    - <font color=\"red\">If this assumption is not met, we can perform different tests that bypass this particular assumption.</font>\n",
    "\n",
    "These ensure that our sample proportions come from a normal distribution. \n",
    "\n",
    "How we check a __large enough sample size__ is we want to see if $n \\dot p$ is at least 10, and $n \\dot (1-p)$ is at least 10.  So, it's like were there at least 10 people that said yes, and were there at least 10 people that said no to the question.\n",
    "\n",
    " __Of course we don't know what p is exactly__. \n",
    " \n",
    " So, instead of using $p$ we're going to use a pseudo p, which is $p_0$ and that is the null population proportion. So, under the null hypothesis, we believe our population proportion to be this, which for our case is 0.52 and our n again was 1,018.\n",
    " \n",
    "So now, we check these assumptions:\n",
    "- our random sample in our problem statement was given to us so we've got a nice check there\n",
    "    - <font color=\"red\">Really, this does need to be told to you. There aren't ways once you've been giving the data, that you can go back through and check for that. So, in essence, we need to know from that they have collected one random sample.</font>\n",
    "- $n \\dot p_0$ = 529 which is lot bigger than 10\n",
    "- $n \\dot (1-p_0)$ = 489 which is lot bigger than 10\n",
    "\n",
    "__So now, we've set up our hypotheses, we've checked our assumptions, and now we can go on to actually running the tests and seeing if we have significant results or not.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"center\"><font color=\"red\">lets do the hypothesis test</font></h4>\n",
    "$$H_0: p=0.52$$\n",
    "$$H_a: p>0.52$$\n",
    "\n",
    "$$\\alpha=0.05$$\n",
    "Sample size of the poll, $n=1018$\n",
    "\n",
    "From the poll we conducted of 1018, __Best Estimate of $p$ is $\\hat{p} = 0.56$__\n",
    "\n",
    "Where $p$ is the population proportion of parents with a teenager who believe that electronics and social media is the cause of their teenager's lack of sleep\n",
    "\n",
    "we now calculate the test statistic:\n",
    "$$Test Statistic = \\frac{Best \\ estimate - Hypothesized \\ estimate}{Standard \\ error \\ of \\ estimate}$$\n",
    "\n",
    "$$Best \\ estimate = \\hat{p}=0.56$$ \n",
    "$$Hypothesized \\ estimate = p_0=0.52$$\n",
    "\n",
    "$$ standard \\ error \\ of \\ estimate= \\frac{\\hat{p}-p_0}{s.e.(\\hat{p})}$$\n",
    " \n",
    "$$s.e.(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}$$\n",
    "but since we don't know $p$, we use $p_0$ instead\n",
    "$$s.e.(\\hat{p}) = \\sqrt{\\frac{p_0(1-p_0)}{n}} = Null \\ s.e.(\\hat{p})$$\n",
    "\n",
    "so, $s.e.(\\hat{p})$ is the __null standard error__ i.e. under the null hypothesis, $s.e.(\\hat{p})$ is the standard error for the $\\hat{p}$\n",
    "\n",
    "$$ standard \\ error \\ of \\ estimate= \\frac{\\hat{p}-p_0}{s.e.(\\hat{p})} = Z \\ Statistic$$\n",
    "\n",
    "Null $s.e.(\\hat{p})$ = 0.0157\n",
    "\n",
    "we get a Z test statistic of 2.555. \n",
    "\n",
    "And so this test statistic, I call it a Z test statistic because we are under a normal distribution, and so whenever we have proportions, it's going to follow a normal distribution, again as long as our sample size is large enough. This Z test statistic means that our observed sample proportion is, 2.555 null standard errors above our hypothesized population proportion\n",
    "\n",
    "The Z test statistic, is just another type of random variable. It has its own distribution. That distribution is going to follow a $N(0,1)$ (normal with mean 0 and std of 1) and the reasoning why it's $N(0,1)$ is our original data was normal, and now we've just centered and scaled that original data.\n",
    "\n",
    "$$ Z \\ test \\ statistic \\ distribution \\ is \\ N(0,1)$$\n",
    "_the test statistic distribution is not always going to be $N(0,1)$ Z test statistic. If the sample size is small, it will be a T test statistic_\n",
    "\n",
    "Now, lets get the $p \\ value$ from this Z statistic. You can either use a Z table chart or use a python function for get the $p \\ value$ for Z statistic = 2.555\n",
    "\n",
    "I get the p-value of 0.0053 for Z statistic of 2.55.\n",
    "\n",
    " Now, that we have our p-value, we can come up with a conclusion with it. So, we have a p-value 0.0053, and we noticed that's less than $\\alpha$ we said earlier in the hypothesis. So because it's less than our $\\alpha$ of 0.05, that means we will reject the null hypothesis, that p equals 0.52. So, basically we got a result that was very unlikely to occur. So, now our initial assumption, we're starting to not really believe it quite as much. \n",
    " \n",
    " __And now that we've said we're going to reject the null, we just want to provide one summary conclusion statement, and that would be something that looks like this.__\n",
    " \n",
    "<h4 align=\"center\"><font color=\"red\">Conclusions</font></h4>\n",
    "\n",
    "$$p-value = 0.0053 < \\alpha = 0.05$$\n",
    "$$Reject \\ the \\ null \\ hypothesis \\ (H_0:p=0.52)$$\n",
    "There is sufficient evidence to conclude that the population proportion of parents with a teenager who believe that electronics and social media is the cause for lack of sleep is greater than 52%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis test of Difference in population proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We're going to be talking about doing a test of differences in population proportions.\n",
    " \n",
    " To start with, we will be looking at data that was collected by CS Mott Children's Hospital. Now, they collect a lot of national polls on issues related to children's health and safety, and this particular one is talking about water safety and in particular looking at swimming lessons. \n",
    " \n",
    " ## setting up the test\n",
    " \n",
    " ### Research Question:  Is there a significant difference between the population proportions of parents of black children and parents of Hispanic children who report that their child has had some swimming lessons?\n",
    " \n",
    " To start, let's think about what our populations are. In this case, our population will be all parents of black children age 6-18. We have a second population, the population of all parents of Hispanic children age 6-18. Now, our parameter of interest will be $p_1 - p_2$. So, the difference in those two population proportions. We'll let 1=black and 2 = Hispanic\n",
    " \n",
    " Populations: All parents of black children age 6-18 and all parents of Hispanic children age 6-18\n",
    " \n",
    " Parameter of interest: $p_1 - p_2$\n",
    " \n",
    " Test for a significant difference in the population proportions of parents reporting that their child has had swimming lessons at the 10% significance level\n",
    " \n",
    " ### setting up the hypotheses\n",
    " <h3><font color=\"green\">Our null hypothesis always contains an =</font></h3>\n",
    "$$H_0: p_1-p_2=0$$\n",
    "$$H_a: p_1-p_2?0$$\n",
    "\n",
    "Typically, what we want to show is in our alternative hypothesis $p_1 - p_2$ and we want to relate it to zero. Now, can't have an equal sign but it can have a greater than, less than, or does not equal. Depending on if we want to see if one group has a higher proportion, a lower population proportion, or just if the two population proportions are not equal. Do you think that the symbol that should replace that question mark is a greater than, less than, or does not equal? Now, in this case, it should be a does not equal because we are looking to see if there is a difference in our population proportions. So, we don't care what direction it is. We just want to see if there is a difference. \n",
    "\n",
    "$$H_0: p_1-p_2=0$$\n",
    "$$H_a: p_1-p_2\\ne0$$\n",
    "$$\\alpha=0.10$$\n",
    "\n",
    "### looking at data\n",
    "Survey Results\n",
    "- a sample of 247 parents of black children age 6-18 was taken with 91 saying that their child has had some swimming lessons.\n",
    "- a sample of 308 parents of Hispanic children age 6-18 was taken with 120 saying that their child has had some swimming lessons.\n",
    "\n",
    "### checking assumptions\n",
    "- We need to assume that we have __two independent random samples__\n",
    "    - <font color=\"red\">Really, this does need to be told to you. There aren't ways once you've been giving the data, that you can go back through and check for that. So, in essence, we need to know from that they have collected two independent random samples.</font>\n",
    "- We also need __large enough sample sizes__ to assume that the distribution of our estimate is normal. That is, we need $n_1\\hat{p},n_1(1-\\hat{p}),n_2\\hat{p},n_2(1-\\hat{p})$ to all be at least 10\n",
    "    - <font color=\"red\">If this assumption is not met, we can perform different tests that bypass this particular assumption.</font>\n",
    "\n",
    "Here $\\hat{p}$ is the __common sample proportion__.\n",
    "\n",
    "$$\\hat{p}=\\frac{91+120}{247+308}=\\frac{211}{555}=0.38$$\n",
    "\n",
    "so, 0.38 of the total proportion of combined population said yes.\n",
    "\n",
    "$$n_1\\hat{p}=94,n_1(1-\\hat{p})=153,n_2\\hat{p}=117,n_2(1-\\hat{p})=191$$\n",
    "so, all the 4 values are greater than 10.\n",
    "\n",
    "### doing the calculations\n",
    "#### Best estimate of the parameter\n",
    "Lets first calculate the best estimate of the parameter. Remember, our parameter is the difference in proportions $p_1-p_2$ and our best estimate will be $\\hat{p_1}-\\hat{p_2}$\n",
    "$$\\hat{p_1}=\\frac{91}{247}=0.37 \\ \\ \\ 1=black$$\n",
    "$$\\hat{p_2}=\\frac{120}{308}=0.39 \\ \\ \\ 2=Hispanic$$\n",
    "$$\\hat{p_1}-\\hat{p_2}=0.37-0.39=-0.02$$\n",
    "\n",
    "and from the Hypotheses we setup earlier, we know:\n",
    "$$H_0: p_1-p_2=0$$\n",
    "$$H_a: p_1-p_2\\ne0$$\n",
    "$$\\alpha=0.10$$\n",
    "\n",
    "also:\n",
    "$$common \\ sample \\ proportion \\ = \\hat{p} = 0.38$$\n",
    "\n",
    "we now calculate the test statistic:\n",
    "$$Test Statistic = \\frac{Best \\ estimate - Hypothesized \\ estimate}{Standard \\ error \\ of \\ estimate}$$\n",
    "\n",
    "$$Best \\ estimate = \\hat{p_1}-\\hat{p_2}=-0.02$$ \n",
    "$$Hypothesized \\ estimate = 0$$\n",
    "\n",
    "$$ standard \\ error \\ of \\ estimate= \\frac{(\\hat{p_1}-\\hat{p_2})-0}{s.e.(\\hat{p})}$$\n",
    " \n",
    "$$s.e.(\\hat{p}) = \\sqrt{\\hat{p}(1-\\hat{p})\\bigg(\\frac{1}{n_1}+\\frac{1}{n_2}\\bigg)} = 0.041$$\n",
    "\n",
    "so, $s.e.(\\hat{p})$ is the __null standard error__ i.e. under the null hypothesis, $s.e.(\\hat{p})$ is the standard error for the $\\hat{p}$\n",
    "\n",
    "$$ standard \\ error \\ of \\ estimate= \\frac{-0.02-0}{0.041} = -0.48 = Z \\ Statistic$$\n",
    "so $z = -0.48$\n",
    "\n",
    "so that means that our observed difference in sample proportions is 0.48 estimated standard errors below our hypothesized mean of equal population proportions. \n",
    "\n",
    "That gives us a sense of where our sample falls. But let's look at what the p-value is and what that means. \n",
    "\n",
    "Because we have a z-test statistic, that will be distributed according to a standard normal distribution $N(0,1)$. In other words, it's a normal distribution. So, it'll be a bell-shaped distribution with the mean at zero and a standard deviation of one.\n",
    "\n",
    "We can directly read the p-value from this distribution. Use either a program or table to get the p-value.\n",
    "p-value = 0.63\n",
    "\n",
    "$$p-val = 0.63 \\gt 0.10=\\alpha \\rightarrow fail \\ to \\ reject \\ null \\ hypothesis$$\n",
    "Note: This does not mean that we accept the null hypothesis. Just that we cannot reject the null hypothesis.\n",
    "$$\\rightarrow don't \\ have \\ evidence \\ against \\ equal \\ population \\ proportions$$\n",
    "\n",
    "<h4 align=\"center\"><font color=\"red\">Conclusions</font></h4>\n",
    "\n",
    "$$p-val = 0.63 \\gt 0.10=\\alpha$$\n",
    "$$Cannot \\ Reject \\ the \\ null \\ hypothesis \\ (H_0: p_1-p_2=0)$$\n",
    "Based on our sample and our p-value, we fail to reject the null hypothesis. We conclude that there is no significant difference between the population proportion of parents of black and Hispanic children who report their child has had swimming lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis test of One Mean: One mean Hypothesis test\n",
    "\n",
    "## Research Question: Is the average cartwheel distance, in inches for adults more than 80?\n",
    "\n",
    "We can take the sample data. Get best estimate of the mean and create confidence intervals around it (with chosen alpha). That is one way of looking at the problem as it will give us a range of possible values for the population mean.\n",
    "\n",
    "Another approach is through Hypothesis testing. We look at the question. Frame it carefully into a hypothesis test. And do the hypothesis test.\n",
    "\n",
    "The null theory will be that the population mean cartwheel distance is indeed 80 inches, and the alternative, the researcher's question of interest is that the mean for this population might be greater than 80 inches.\n",
    "$$H_0: \\mu=80$$\n",
    "$$H_a: \\mu > 80$$\n",
    "$$\\alpha=0.05$$\n",
    "\n",
    "__Note:__ We really could write these as the alternative being greater than 80 and that null hypothesis being less than or equal to 80 because that's really what we are testing. But if we end up showing that our data supports that we're greater than 80, that data is also going to support that were much greater than 79 or 78 or anything smaller. So we really only have to be concerned with that 80 as our null value, are we significantly more than 80 inches? \n",
    "\n",
    "\n",
    "- __Step one__: The standard significance level is 5%, which is basically establishing that the results need to be so unusual under that null theory that we would see those results are more extreme, no more than 5% of the time. So, if the results are more extreme or more unusual than that 5%, we will reject that null and go with the alternative theory. \n",
    " \n",
    "- __Step two__: Start looking at our data, examine the results, check some assumptions, and then summarize our data through a statistic. So, here's the numerical summaries of our 25 cartwheel distance measurements. Of interest here is that the mean of our sample is 82.48 inches. Certainly that looks to be more than 80, but our question will be is it significantly more than 80? We do know that that mean of 82.48 inches is our mean of our sample and if we were to repeat the study again with another 25 adults, we might get a slightly different answer. So, we have to establish whether that 82.48 is significantly larger than the 80 inches in our null theory.\n",
    "    - We know that if we were to do the study again we would get different sample mean values out that even if the null hypothesis were true and the true mean was really 80 inches, we will tend to get sample means that vary around it, sometimes above, sometimes below. So, is this difference that we're seeing out here at 82.48 inches significant or is it just a value that's reflecting that natural variability in the sample mean? We will be able to answer this question by calculating standard error of sample mean and then take the ratio of the mean with sem. We will do this later.\n",
    "\n",
    "- __Step three__: Checking assumptions. we should also examine some of the assumptions. The main assumptions for doing the test for a one-sample t-test for mean is that the sample could be considered a random sample and then that the model for our population of all cartwheel distances can be assumed to be bell-shaped or normal. Now that normality condition is not quite as crucial if our sample size is large enough because we'll be able to rely on that Central Limit theorem, but we should still graph our data and take a look at what it wants to say to us. So, here is a histogram and a Q-Q plot of our 25 cartwheel distance measurements. And both of these graphical summaries are showing some modest deviations from that bell curve or normality condition. However, we do have 25 observations, that's a reasonable sample size starting to be somewhat large enough so that we could say that CLT, that Central Limit Theorem is still going to allow us to have the model for the sample means be approximately normal, so that initial normality condition for the original population isn't quite so crucial.\n",
    "\n",
    "- __Step four__:calculate the test statistic for our hypothesis.\n",
    "\n",
    "    $$standard \\ error \\ of \\ sample \\ mean = \\frac{estimated \\ sample \\ standard \\ deviation}{\\sqrt{n}}$$\n",
    "\n",
    "    using numpy, you can direclty get it by np.sem() for standard error of sample mean and np.std() for estimated sample standard deviation.\n",
    "\n",
    "    We will look at our best estimate, our sample mean. We'll compare it to the null value of 80 to see how far away it is from 80, and we'll use in the denominator, the yardstick that we can measure and get from our sample that estimated standard error. We had 82.48 inches as our sample mean, and it was a little less than 2.5 inches above that 80, and with the right yardstick in the bottom is that estimated standard error. \n",
    "\n",
    "    $$standard \\ error \\ of \\ sample \\ mean = \\frac{sample \\ standard \\ deviation}{\\sqrt{n}} = 0.82 = test-statistic$$\n",
    "\n",
    "    This test statistic, the standardized quantity comes out to be 0.82. Think about how we might interpret that 0.82 as our test statistic value. Our test statistic, our t statistic turned out to be 0.82. If we look at the structure of how that statistic was formed, it is looking at the distance on the top, how our sample mean compared to the hypothesized or null value of 80 in terms of estimated standard errors on the bottom.\n",
    "\n",
    "- __Step five__: Determine if the null hypothesis were true.\n",
    "    First, we need to know what model our test statistic follows, and t test statistics end up following a t distribution with a certain degrees of freedom to index which distribution we're working with. T distributions are still somewhat bell-shaped, they just have a little thicker tails than our normal distributions we were working with in the past for testing about proportions, and the degrees of freedom indicate how thick those tails are. The other thing to keep in mind is this idea of what is more extreme. Since our theory was that we might be bigger than 80 inches our direction was to the right, the more extreme will be measured as large or larger in that upper tail (right hand tail). So, let's look at a picture of that P-value. Here is a t distribution with 24 degrees of freedom, 25 minus one, and along the axes, we have various test statistic values that are possible. Our test statistic of 0.82 is noted, and we have shaded here the probability that represents this P-value. How likely is it to get 0.82 or something more extreme, larger? Well, that probability is actually pretty high, it's 0.21, little more than 20 percent of the time, you would see the result we got or something more extreme if $H_0$ were really true. It isn't that unusual. Seeing the results we got is actually quite likely under that null theory. \n",
    "    \n",
    "So, we take this probability value (p-value) that's now in a probability scale, and we make an evaluation of it to make our decision in the same way for all tests. If our P-value is big compared to our significance level, we do not have enough evidence to reject the null. We say we fail to reject that null theory. If the P-value turned out to be small less than or equal to the significance level, saying that our data was more unusual than what we required it to be under h naught to convince us to reject it, we'll then we'd reject $H_0$.\n",
    "\n",
    "<font color=\"green\">Our conclusion might be written that based on our estimated sample mean of 82.48 inches, we're not able to support the theory that the population mean cart will distance is greater than 80 inches.</font>\n",
    "\n",
    "__Confidence intervals give us a range of reasonable values for our parameter. So, we're going to look at a confidence interval at the 90 percent level of confidence. Because if we had 90 percent as our area in between that is going to leave five percent in each tail, total of 10 and we're doing a test of hypotheses with a five percent significance level in our upper tail. So, the appropriate multiplier to give us that 90 percent level of confidence will be used and the confidence interval turns out to go from 77 to a little over 87 inches. This is a range of reasonable values for what we think the population mean might be. Now, our theory was that the population mean might be equal to 80 inches or is it significantly greater. Even though our sample mean was greater than 80 inches, when we extend and allow for the variability that sample means will have, we dip down below that 80 inches. The 80 inches is inside our confidence interval of reasonable values for this population mean cart will distance. We cannot refute that 80 and support that it's significantly greater than 80 because the entire interval is not above the 80 inches.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non parameteric approaches for hypothesis test\n",
    "\n",
    "If we're not convinced that the normal model is reasonable and we're a little skeptical about our sample size being large enough, there are alternate techniques that we could use. Non-parametric test that don't assume normality. One of those analogues for the one sample t test we performed is called the __Wilcoxon Signed Rank Test__, which will use the median as it's measure of center location rather than the mean. In performing that test, we would calculate a test statistic, and we would get out another P-value and use that to make the decision in the same way we have in the past. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a Population Mean Difference (extension of One mean Hypothesis test) - Paired data\n",
    "(looking at paired data)\n",
    "\n",
    " we'll be looking at home renovations. We have 20 homes that are remodeling their kitchens and requesting cabinet quotes from two suppliers. We're interested in knowing, is there an average difference in cabinet quotes from these two suppliers.\n",
    " \n",
    "  In this case, because we have two measurements taken for each home, we have two measurements on the same individual. So, we're paired and matched based on the individual, based on the homes.\n",
    "  \n",
    "__Variable__: Difference in cabinet quotes (SupplierA - SupplierB) \n",
    "\n",
    "__Research Question__: Is there an average difference between the cabinet quotes from the two suppliers.\n",
    "\n",
    "__Population__: All houses\n",
    "\n",
    "__Parameter of interest__: Population mean difference of cabinet quotes $\\mu_d$. Difference is (SupplierA - SupplierB)\n",
    "\n",
    "__$\\alpha$__: test for a significant mean difference in cabinet quotes at the 5% significance level\n",
    "\n",
    "So, summarizing:\n",
    "$$H_0: \\mu_d=0$$\n",
    "$$H_a: \\mu_d \\neq 0$$\n",
    "$$\\alpha=0.05$$\n",
    "\n",
    "Note: In this case, we're looking at \"does not equal zero\" since we don't have a clear indication of which supplier we expect to be more expensive or less expensive. So we're just looking to see is there any difference, if the mean is different from zero. Not necessarily looking for a specific side. \n",
    "\n",
    "__Plots__: We can make a SupplierA vs SupplierB scatterplot to see the correlation of data. From this scatterplot, it will be difficult to tell which supplier is more expensive but we will get an idea if they are correalted.\n",
    "\n",
    "__Assumptions__:\n",
    "we need to assume that we have a random sample of differences i.e. a random sample of houses that we're looking at. Now we are looking at those differences within those houses, but, having a random sample of houses should also indicate that we have a random sample of differences and those two cabinet quotes. We also need to have the population of differences be normally distributed. So we'd like the population of differences to be normally distributed. We can get put assumption if we have a large sample size about 25 or more. In this case we only have 20 observations. So, we'll look to see if it seems reasonable that the population of differences is normally distributed. We can look at a normal Q-Q plot. Typically you would hope to see that all of our points fall along a straight line. In this case, the line wouldn't be perfectly straight that connects all of these points, but in general, the line should be relatively straight, and given that we have 20 observations that's close to that 25, plus our points fall roughly along that straight line, this is a reasonable assumption. We can go ahead and assume that we have the population of differences normally distributed.\n",
    "\n",
    "__More plots__: we can look at this histogram of the differences in cabinet quotes, keeping in mind that because we have paired data, we're always looking at that difference variable. We see that it does look roughly bell-shaped, it's not perfectly bell-shaped, but does look roughly bell-shaped.\n",
    "\n",
    "__Calculate summary statistics of differences__:\n",
    "\n",
    "Using all of those summary statistics, we can go ahead and perform the test for the population mean difference. \n",
    "\n",
    "__So to start with, we'll think about calculating a test statistic. If we assume that the sampling distribution of the sample mean difference is normal, then we can use that t-test statistic.__\n",
    "\n",
    "$$t = \\frac{best \\ estimate - hypothesized \\ estimate}{estimated \\ standard \\ error \\ of \\ estimate}$$\n",
    "$$t = \\frac{\\bar{x}_d - 0}{s_d/\\sqrt{n}} = \\frac{17.30 - 0}{28.49/\\sqrt{20}} = 2.72$$\n",
    "\n",
    "Lets interpret this test statistic: This means that our observed mean difference is 2.72 estimated standard errors above our null value of zero or our hypothesized value of zero. This is quite high, __but in order to tell exactly how high that is, we need to look at what the p-value is__.\n",
    "\n",
    "Because we have a t-test statistic, we know that it'll follow a t distribution, and in this case it'll have 19 degrees of freedom. We have 20 observations, you can take 20 minus one to be 19 and that's how we get the 19. \n",
    "\n",
    "In this case, because our alternative hypothesis is $\\my_d \\neq 0$, a three is just as extreme as a negative three, it's just as far away from zero. So we'll shade both sides. We'll shade the area above that 2.72, and the area below the negative 2.72. Now you can go ahead with technology or with a table to look and calculate what that area is that we've just shaded. This will be our p-value, in this case our p value is going to be 0.014, that means that we would observe a test statistic as extreme or more extreme, only about 1.4 percent of the time. So it's pretty unlikely that we'd get a test statistic that large. Now using our p-value of 0.014 we should compare that to our Alpha or significance level of 0.05. We see that point 0.014 is less than 0.05. So that indicates that we should reject the null hypothesis, we have enough evidence against the null hypothesis that we can reject it.\n",
    "\n",
    " __Formally__: Based on our sample and our p-value, we reject the null hypothesis and conclude that the mean difference of cabinet quote prices for supplier A less supplier B is significantly different from zero. \n",
    " \n",
    " Now if we wanted to get a sense of in what way it's different from zero, we might look at a 95 percent confidence interval.\n",
    " \n",
    " $$\\bar{x}_d \\pm t^\\ast\\bigg(\\frac{s_d}{\\sqrt{n}}\\bigg)$$\n",
    " \n",
    " sp, our confidence interval is: ($3.97,$30.63). __Note__: 0 is not in the range of resonable values for mean difference in cabinet prices. Because all of our values are positive, that also indicates that supplier A is probably more expensive than supplier B. But because zero is not in that range of reasonable values that indicates that the mean difference is reasonably not going to be zero.\n",
    " \n",
    " __if normality didn't hold, that assumption of normality, that the population of all differences is normal, we can use the Wilcoxon Signed Rank Test.__ p-value computed from Wilcoxon Signed Rank Test is 0.02. In this case again, we're going to reject the null hypothesis, and will conclude that the median difference in cabinet quotes supplier A less supplier B is different from zero.\n",
    " \n",
    " \n",
    "# Testing a Population Mean Difference (extension of One mean Hypothesis test) - Independent groups\n",
    "\n",
    "__Research Question__: Considering Mexican-American adults ages 18 to 29 living in the United States, do males have a significantly higher mean body mass index than females? \n",
    "\n",
    "__Population__: Mexican american adults ages 18-29 in the US\n",
    "\n",
    "__Parameter of interest__($\\mu_1-\\mu_2$): Body mass index or BMI ($kg/m^2$)\n",
    "\n",
    "__Task__:We're going to perform today is an independent samples t-test regarding the value for the difference in mean BMI between males and females. We're going to actually see if we have a significant difference between these two populations.\n",
    "\n",
    "__Steps to perform a hypothesis test are as follows__:\n",
    "- First, we are going to define the null and alternative hypothesis. \n",
    "- Then, we're going to go through examine our data, check our assumptions and make sure those hold\n",
    "- then calculate a test statistic for our data. With that test statistic, we're going to determine the corresponding p-value\n",
    "- finally, make a decision based off this p-value about our null hypothesis. \n",
    "\n",
    "\n",
    "$$H_0: \\mu_1 - \\mu_2 = 0$$\n",
    "$$H_a: \\mu_1 - \\mu_2 \\neq 0$$\n",
    "$$\\alpha=0.05$$\n",
    "\n",
    "Then look at the summary statistics for the data.\n",
    "\n",
    "Then check the assumptions.\n",
    "- There are more complex sampling designs out there but again, here, we're just going to assume that they're simple random samples. We need to make sure that the samples are independent from one another. We need to make sure that those datas for males and females aren't matched up specifically or can't be paired in any way. Here, all we've done is split the data based on gender so we have no reason to believe that they're dependent.\n",
    "- we need to make sure that both populations of responses are approximately normal. So, let's check that now with our data. On the left side, we have our histograms for both male and female and on the right side, we have our q-q plots or quantile-quantile plots for males and females. The graph on the left, we want to look bell-shaped to represent a normal distribution. The graphs on the right, we really want the data points to follow that line running down the middle. That line represents a normal distribution so the closer those data points are, the more normal our distribution appears. Here, we can see that there's quite a bit of deviation for the female group especially in that bottom right plot, a little bit less so for the male group in the q-q plot. So, it's a little bit closer to normal. But we do have very large sample sizes for both these groups, over 200. So, this normality assumption isn't as crucial. We can apply the central limit theorem and move on anyways.\n",
    "\n",
    "__in running a hypothesis tests for two means, the results are fairly robust if you have a large sample size. So, we're going to go ahead and move forward without this normality assumption because our sample size of over 200 is fairly large.__\n",
    "\n",
    "Lets calculate the test statistic:\n",
    "__Best Estimate__:$\\bar{x}_1 - \\bar{x}_2 = 23.57 - 22.83 = 0.74$\n",
    "\n",
    "Is our sample mean difference of 0.74 $kg/m^2$ significantly different than 0?\n",
    "\n",
    "$$t = \\frac{best \\ estimate - hypothesized \\ estimate}{estimated \\ standard \\ error \\ of \\ estimate}$$\n",
    "we can write the same thing as:\n",
    "$$t = \\frac{best \\ estimate - null \\ value}{estimated \\ standard \\ error \\ of \\ estimate}$$\n",
    "\n",
    "__Remember, the estimated standard error for two means can change depending on which approach we're going to use. The two approaches that you can use are the pooled approach and the unpooled approach. The pooled approach can be used if the variance of the two populations are assumed to be equal.__\n",
    "\n",
    "#### Pooled Approach for two means confidence interval\n",
    "This is where our variances are closer enough in our samples that we can assume that our variances for the populations are similar, and then we pool that variance together. So again we have that same difference in sample means, and then we're going to plus or minus a few estimated standard errors. The estimated standard error for pooled is going to change now. So instead of having that original calculation, we now have our pooled standard deviation out in front and we multiply that by the square root of the inverse of our sample sizes.\n",
    "\n",
    "with Pooled approach, the estimated standard error is given by:\n",
    "\n",
    "__Estimated Standard Error__ = $\\sqrt{\\frac{(n_1-1)s_1^2 \\ + \\ (n_2-1)s_2^2}{n_1+n_2-2}} \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}$\n",
    "\n",
    "\n",
    "\n",
    "#### lets use the unpooled approach since the two stds appear to be slightly different\n",
    "\n",
    "with Unpooled approach, the estimated standard error is given by:\n",
    "\n",
    "__Estimated Standard Error__  = $\\sqrt{\\frac{s_1^2}{n_1} \\ + \\ \\frac{s_2^2}{n_2}}$\n",
    "\n",
    "so we compute our test statistic to be t= 1.30 with pooled approach\n",
    "\n",
    " t=1.30 tells us that our difference in sample means is only 1.30 estimated standard errors above our null difference of zero kilograms per meter squared. But now, we want to know is this far enough where we should reject our null value of zero or is it not? \n",
    " \n",
    " So, if the null hypothesis was actually true, if those population means were zero, would a test statistic of 1.30 be unusual enough to reject the null? With that, we're going to calculate our p-value. \n",
    " \n",
    " What is our p-value? It's the probability of observing a test statistic of 1.3 or more extreme. \n",
    " \n",
    " Here, all of this is going to be done again assuming that that null hypothesis is true and that those population means are actually equal. \n",
    " \n",
    " So, we're going to calculate this probability using a t-distribution where the degrees of freedom for this distribution are n1 plus n2 minus two. \n",
    " $$df = n_1+n_2-2$$\n",
    " \n",
    " When we draw this out, we're going to have to check both sides. This is a two-sided alternative hypothesis because our alternative is not equal to. So, we have to check both the upper and lower tail of our distribution. \n",
    " \n",
    " p-value = 0.1942\n",
    " \n",
    " So, this means that if the difference in population mean BMI between males and females was really zero, so if that null hypothesis was true, then observing a difference in sample means of 0.74 or something more extreme is fairly likely. There's almost a 20 percent chance of seeing that. So, because this value is so large, we are going to go ahead and fail to reject the null.  It's larger than our significance level, so there's very weak evidence against this null hypothesis that the means are actually equal.\n",
    " \n",
    " __Conclusion__: Based on our estimated difference in sample means, we cannot support that there is a significant difference between the population mean BMI for males and the population mean BMI for females for a given population here which is Mexican-American adults living in the United States ages 18 to 29.\n",
    " \n",
    " If we think back to a prior lecture where we created the confidence interval for these results for this data, we see that our 95 percent confidence interval was __(-0.385, 1.865)__. Our null hypothesized test value was zero kilograms per meter squared and that falls within our interval. Remember that this interval is a range of reasonable values for that true population mean difference so because zero is contained in our interval, it seems like it could be a reasonable value and we find that our confidence intervals agree with our hypothesis tests results. \n",
    " \n",
    " \n",
    "# Summarizing:\n",
    "### hypothesis tests are used to put theories about a parameter of interest to the test. \n",
    "Here, that parameter is the difference in population means. \n",
    "\n",
    "### The basic steps for performing this hypothesis test. \n",
    "- First, we're going to define our hypotheses. \n",
    "- Then, we're going to examine our data while checking our assumptions and calculating our test statistic. \n",
    "- With this test statistic, we'll determine our corresponding p-value, \n",
    "- and then finally, we will make a decision based on this value. \n",
    "\n",
    "### The assumptions for the two sample (t) Test for population means:\n",
    "- we need both sets of data to be two simple random samples and they need to be independent from one another. \n",
    "- We need to make sure that both populations of responses are normally distribute. \n",
    "    - If not, we need to make sure we at least have a large sample size so we can apply the central limit theorem. \n",
    "    \n",
    "#### Whether or not our population variances are equal is also crucial in determining whether we use a pooled or unpooled approach. \n",
    "Finally, we need to know how to interpret the p-value, the decision, and our final conclusion. These are all very important when conducting a hypothesis test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"green\">Hypothesis Testing: Other Considerations</font></h1>\n",
    "\n",
    "Statistical hypothesis testing reflects the scientific method, adapted to the setting of research involving data analysis. In this framework, a researcher makes a precise statement about the population of interest, then aims to falsify the statement. In statistical hypothesis testing, the statement in question is the null hypothesis. If we reject the null hypothesis, we have falsified it (to some degree of confidence). According to the scientific method, falsifying a hypothesis should require an overwhelming amount of evidence against it. If the data we observe are ambiguous, or are only weakly contradictory to the null hypothesis, we do not reject the null hypothesis.\n",
    "\n",
    "The framework of formal hypothesis testing defines two distinct types of errors. A type I error (false positive) occurs when the null hypothesis is true but is incorrectly rejected. A type II error occurs when the null hypothesis is not rejected when it actually is false. Most traditional methods for statistical inference aim to strictly control the probability of a type I error, usually at 5%. While we also wish to minimize the probability of a type II error, this is a secondary priority to controlling the type I error.\n",
    "\n",
    "All the standard statistical testing procedures perform well at controlling type I error under ideal conditions, but most of them can break down and give misleading results in practice. That is, if we claim to be conducting a test that has a 5% false positive rate, is this the actual false positive rate? As we discussed earlier in the setting of confidence intervals, several complications that can arise in practice will result in a statistical procedure not performing as intended. In order to reduce the risk of this happening, statisticians use statistical theory and computer simulations to assess the operating characteristics of testing procedures in various challenging settings.\n",
    "\n",
    "## Normality of the data\n",
    "One of the commonly-stated “assumptions” that is often raised as a caveat when presenting statistical findings is the issue of the data being normally distributed. While it is true that in some circumstances, strongly non-normal data can cause statistical tests to be misleading, in most settings, the data are not required to follow a normal distribution. In addition, there are other issues unrelated to normality that are potentially more likely to produce misleading results.\n",
    "\n",
    "Concerns about normality primarily center on the calibration of rejection regions for a test statistic, or equivalently, the manner in which p-values are computed. As with confidence intervals, the Z-score plays the central role. To be concrete, suppose we are conducting a test comparing two population means using independent samples (a “two sample t-test”). The corresponding sample means are x1_bar and x2_bar, and the test is based on the difference between them, which is x1_bar - x2_bar. This difference has a standard error, which we denote here by s (there are a few different ways to compute this standard error, s can refer to any of them here). The Z-score is (x1_bar - x2_bar) / s.\n",
    "\n",
    "Under the very strong assumption that the data are normally distributed, the Z-score follows a Student t-distribution, with degrees of freedom depending on the way that the standard error was constructed (in most cases the degrees of freedom will be between m-2 and m, where m is the combined sample size of the two samples being compared). If the data are not normally distributed, we can appeal to the central limit theorem (CLT), which states that the Z-score will be approximately normally distributed as long as the sample size is not too small.\n",
    "\n",
    "As discussed earlier in the setting of confidence intervals, there is no universal rule that states when the sample size is large enough to justify invoking the CLT. Rules of thumb between 20 and 50 are often stated. A smaller sample size is sufficient to invoke the CLT when the data approximately follow a normal distribution, and a larger sample size is needed if the data are strongly non-Gaussian.\n",
    "\n",
    "While normality is a consideration in some settings, it is mainly relevant when the sample size is small and the data are strongly non-Gaussian. Other issues can cause statistical tests to break down in a broader range of settings, so normality of the data should be seen as one of several factors that can impact the performance of a statistical test, and is often a relatively minor one at that.\n",
    "\n",
    "A useful approach in practice is to use the Student t-distribution to calculate rejection regions and p-values, even when the data are not expected to follow a normal distribution. Doing so is slightly conservative, in that the rejection region based on the t-distribution will be slightly smaller than the rejection region based on the normal distribution, and p-values based on the t-distribution will be slightly larger than p-values based on the normal distribution. As the sample size grows beyond around 50, there is little practical difference between using the t-distribution and the normal distribution when carrying out statistical hypothesis tests.\n",
    "\n",
    "Clustering and data dependence\n",
    "One additional issue that can adversely impact the performance of a statistical test is the presence of unknown (or unmodeled) correlations or clustering in the data. For example, if the data values are observed in sequence (e.g. over time), with each value possibly being correlated with its neighbors, then we have “autocorrelation”, which is a form of dependence. Alternatively, we may have some form of grouping or clustering in the data. Methods for addressing these issues will be discussed in Course 3 of this specialization.\n",
    "\n",
    "## Causality\n",
    "Another issue to be aware of when conducting statistical tests is that of confounding and causality. This issue is especially relevant when interpreting the results of a statistical test. Suppose, for example, that two groups of people differ significantly in terms of some trait, e.g. people with fewer dental cavities are seen to have statistically lower risk of heart disease compared to people with more cavities. It is important to note that this effect could be due to a lurking factor, or to some form of selection bias. For example, the people with fewer cavities may be less likely to be smokers. Note that this is arguably not a problem with the statistical hypothesis test itself -- it may well be true that people with fewer cavities are less likely to have heart disease (in the sense of there being a real association between these two factors). Rather, it is an issue of what substantive conclusions may be drawn, especially when people draw a causal conclusion where one is not warranted.\n",
    "\n",
    "## Multiplicity\n",
    "A third pitfall that arises with statistical hypothesis testing, as well as with other forms of statistical inference such as confidence intervals, is that of “multiplicity”, which we will discuss next. Note that a variety of terms have been used to describe this issue, including “data dredging”, “multiple testing”, and “p hacking” (in reference to “p values”).\n",
    "\n",
    "Most statistical inference procedures, including both confidence intervals and hypothesis testing, are based on an idealized research design in which a single analysis with narrow scope is conducted using a data set. In practice, data analysis often involves data exploration coupled with formal inference. It is now widely accepted that doing this heedlessly can lead to misleading results, and in particular often leads to statistical evidence for research findings being overstated. This is a large topic; here we comment on a few aspects of it, starting with two examples of how multiplicity in data analysis can cause problems.\n",
    "\n",
    "Suppose that a researcher habitually conducts a two-sample t-test comparing group means, then reports a confidence interval for the difference in population means only if the null hypothesis of the t-test is rejected. On one hand, this researcher is exhibiting good judgment, as it is often recommended to report an effect size along with the results of any hypothesis test (the point estimate of the population mean difference is an effect size in this setting). However, the confidence intervals selected in this way will have lower than the nominal coverage probability. This is an example of a broader issue sometimes called “selective inference”.\n",
    "Suppose that there are natural ways to divide the population into subgroups. For example, imagine that a researcher is interested in whether people who sleep less than 7 hours per night on average during one month have greater gain of body weight in the following year. The researcher’s initial plan may have been to consider the general population, but perhaps the investigator then decides to carry out analyses separately in women and in men, in older people and in younger people, in smokers and in non-smokers, etc. Although it is possible that hypothesized effect may actually be much stronger in some of these subgroups than in others, repeatedly testing the same data on different subgroups will be likely to give rise to falsely positive evidence for an association. For example, if the researcher conducts 3 independent tests on the same data (e.g. on different subgroups of the data), with each test having a 5% false positive probability, then the probability that at least one of the tests will yield a false positive is over 14%.\n",
    "In recent years, researchers have identified more and more ways that multiple-testing can arise in practice, corrupting statistical findings. Almost always, multiple testing leads to overstatements of the confidence in findings, or to erroneous findings being reported. Fortunately, there are many approaches to remedying this issue. The easiest of these to apply is the Bonferroni correction, which essentially involves multiplying all p-values by the number of tests that were performed. For example, if we conduct 5 hypothesis tests, and one of them yields a p-value of 0.02, then we should adjust this p-value to 0.1 (= 0.02 * 5). Thus, this test which would have been deemed to be “statistically significant” if conducted in isolation will be not be seen as such following the Bonferroni adjustment.\n",
    "\n",
    "## Power\n",
    "A final consideration we will discuss here is statistical power. Power is often defined in a narrow sense as the probability of rejecting the null hypothesis when the null hypothesis is false. Loosely speaking, this is the probability of not making a type II error. More broadly, power can refer to any aspect of the study design or data analysis that would make it more likely for meaningful results to be attained. There is a branch of statistics focusing on formal “power analysis” that aims to develop concrete and quantitative ways to assess the statistical power in a given setting. We will not delve into these methods here, and instead will discuss at a higher level how low power intersects with and exacerbates some of the other complicating considerations discussed above.\n",
    "\n",
    "The type I error rate is controlled by the researcher (say at 5%), but this only represents the risk of drawing a false conclusion from a single test. In recent years, the notion of the “false discovery rate” (FDR) has been advanced to understand how often the conclusions drawn in a research process involving multiple formal inferential procedures are mistaken. Focusing on hypothesis tests, we can consider a situation where, for example, five tests are to be conducted. If two of the underlying null hypotheses are false, but the power to reject them is low (say it is only 20%), then around 25% of all the rejected null hypotheses were incorrectly rejected. This shows how the FDR is different than the type I error rate, which remains controlled here at 5%.\n",
    "\n",
    "People sometimes incorrectly believe that controlling the type I error rate at 5% means that there is only a 5% chance that any reported finding is wrong. As illustrated here, the probability that a reported finding is wrong can be much higher than the type I error rate. This “error inflation” is primarily driven by two factors -- a researcher who pursues hypotheses that are unlikely to be correct, and a researcher who carries out studies with low statistical power will both have higher FDR in their work overall. The latter issue is in principle addressable by encouraging researchers to pursue fewer, but higher quality studies (i.e. to pursue fewer studies with larger sample sizes rather than many studies with small sample sizes). The former issue is harder to address, but it reflects the fact that in some fields, especially difficult areas of science such as genetics and neuroscience, there is a poor fundamental understanding of the systems under study, which may lead to people speculating and pursuing hypotheses with weak theoretical grounding.\n",
    "\n",
    "<h1><font color=\"green\">The Relationship Between Confidence Intervals and Hypothesis Testing</font></h1>\n",
    "\n",
    "Confidence intervals and hypothesis tests are both forms of formal statistical inference. There is a strong connection between them. When working with a single parameter, tests and intervals are essentially equivalent, but this equivalence is lost when there is more than one parameter.\n",
    "\n",
    "If we have a confidence interval for a single target parameter, say the population mean (mu), then we can convert this interval into a hypothesis test by declaring that the null hypothesis mu=c is rejected if c is not inside the confidence interval. Here c is a specific numerical value that must be pre-specified. The type I error rate of the resulting test is 1 minus the coverage probability of the confidence interval. Thus, a 95% coverage probability interval is equivalent in this sense to a hypothesis test with 5% type I error rate (i.e. ɑ = 0.05). There are variations on this equivalence for one-sided tests, which are equivalent to one-sided intervals, and for two-sided tests, which are equivalent to two-sided intervals.\n",
    "\n",
    "Going the other way, if we have a hypothesis testing procedure for a parameter mu, then we can conduct a test of the null hypothesis mu=c for all possible values of c, at a fixed type I error rate. The set of all values of c for which the null hypothesis is not rejected is a confidence interval. If the tests are conducted with type I error rate alpha (e.g. ɑ=0.05), then the coverage probability of the resulting interval is 1 - ɑ (e.g. 0.95). This process is often called \"__inverting a hypothesis test to construct a confidence set__\". Note that in practice it is not actually possible to conduct the test for all possible values of c, because there are infinitely many such values. There are numerical algorithms to address this computational issue, but we will not cover them here.\n",
    "\n",
    "More subtle questions arise when we are working with two or more parameters. For example, suppose we have samples of female and male office workers, and are comparing their incomes. We can construct two 95% confidence intervals -- one for the women and one for the men. It is natural to think that if these two intervals do not overlap, then we can reject the two-sample test that the population means are different, and conversely, if the two intervals do overlap, then we do not reject the hypothesis that the population means are different. But in fact, only one of these assertions is true. When the confidence intervals do not overlap, then the two-sample test will always reject its null hypothesis. But the converse is actually not true. There are situations where the confidence intervals overlap, but where the hypothesis test will still reject its null hypothesis.\n",
    "\n",
    "In general, it is safer to conduct a hypothesis test explicitly if that is the type of result that will be reported. But if for some reason it is not possible to conduct the hypothesis test, but the intervals are available (e.g. they appear in a paper but the primary data are not accessible), then it is important to remember that the test results cannot always be inferred from knowing only whether the confidence intervals overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
